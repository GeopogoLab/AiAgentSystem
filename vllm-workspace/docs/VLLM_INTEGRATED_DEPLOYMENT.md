# VLLM é›†æˆéƒ¨ç½²æŒ‡å—

## ğŸ‰ éƒ¨ç½²æ¦‚è§ˆ

å·²æˆåŠŸéƒ¨ç½² **vLLM + FastAPI é›†æˆæœåŠ¡** åˆ° Modalï¼Œé¿å…å†·å¯åŠ¨é—®é¢˜ã€‚

### ğŸ“ æœåŠ¡ä¿¡æ¯

- **æœåŠ¡URL**: `https://ybpang-1--vllm-integrated-serve.modal.run`
- **æ¨¡å‹**: Llama-3.1-8B-Instruct
- **GPU**: 1x A100-80GB
- **é—²ç½®è¶…æ—¶**: 30åˆ†é’Ÿï¼ˆæ— è¯·æ±‚åè‡ªåŠ¨ä¼‘çœ ï¼‰
- **éƒ¨ç½²æ–‡ä»¶**: `modal_vllm_integrated.py`

## âœ¨ æ¶æ„ä¼˜åŠ¿

### ä¸ä¹‹å‰æ–¹æ¡ˆçš„å¯¹æ¯”

**ä¹‹å‰çš„æ¶æ„**ï¼ˆ2ä¸ªç‹¬ç«‹æœåŠ¡ï¼‰:
- vLLMæœåŠ¡å™¨ï¼ˆModalï¼‰
- FastAPI Wrapperï¼ˆæœ¬åœ°æˆ–å•ç‹¬éƒ¨ç½²ï¼‰
- âŒ æ¯æ¬¡è°ƒç”¨éœ€è¦é€šè¿‡ç½‘ç»œ
- âŒ æ¯ä¸ªæœåŠ¡ç‹¬ç«‹å†·å¯åŠ¨
- âŒ ç®¡ç†å¤æ‚

**ç°åœ¨çš„æ¶æ„**ï¼ˆé›†æˆéƒ¨ç½²ï¼‰:
- âœ… vLLMå’ŒFastAPIåœ¨åŒä¸€å®¹å™¨ä¸­
- âœ… é€šè¿‡localhosté€šä¿¡ï¼Œé›¶ç½‘ç»œå»¶è¿Ÿ
- âœ… ä¸€æ¬¡å†·å¯åŠ¨ï¼ŒæœåŠ¡æŒç»­è¿è¡Œ
- âœ… 30åˆ†é’Ÿæ— è¯·æ±‚åè‡ªåŠ¨ä¼‘çœ èŠ‚çœæˆæœ¬
- âœ… ç»Ÿä¸€éƒ¨ç½²å’Œç®¡ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•æœåŠ¡å¥åº·çŠ¶æ€

```bash
curl https://ybpang-1--vllm-integrated-serve.modal.run/health
```

å“åº”ç¤ºä¾‹:
```json
{
  "status": "healthy",
  "vllm_available": true,
  "model": "meta-llama/Llama-3.1-8B-Instruct"
}
```

### 2. å‘é€å¯¹è¯è¯·æ±‚ï¼ˆç®€åŒ–æ¥å£ï¼‰

```bash
curl -X POST https://ybpang-1--vllm-integrated-serve.modal.run/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello! How are you?"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### 3. ä½¿ç”¨OpenAIå…¼å®¹æ¥å£

```bash
curl -X POST https://ybpang-1--vllm-integrated-serve.modal.run/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [
      {"role": "user", "content": "Write a haiku about AI"}
    ],
    "max_tokens": 50
  }'
```

### 4. åœ¨Pythonä¸­ä½¿ç”¨

```python
import httpx
import asyncio

async def call_vllm():
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            "https://ybpang-1--vllm-integrated-serve.modal.run/chat",
            json={
                "messages": [
                    {"role": "user", "content": "Explain quantum computing"}
                ],
                "max_tokens": 200,
                "temperature": 0.7
            }
        )
        result = response.json()
        print(result["content"])

asyncio.run(call_vllm())
```

## ğŸ“¡ APIç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/` | GET | æœåŠ¡ä¿¡æ¯ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/models` | GET | åˆ—å‡ºå¯ç”¨æ¨¡å‹ |
| `/chat` | POST | ç®€åŒ–å¯¹è¯æ¥å£ |
| `/v1/chat/completions` | POST | OpenAIå…¼å®¹æ¥å£ |

## ğŸ”§ é‡æ–°éƒ¨ç½²

å¦‚éœ€æ›´æ–°é…ç½®æˆ–é‡æ–°éƒ¨ç½²:

```bash
cd /path/to/AiAgentSystem
modal deploy modal_vllm_integrated.py
```

## âš™ï¸ é…ç½®é€‰é¡¹

åœ¨ `modal_vllm_integrated.py` ä¸­å¯ä»¥ä¿®æ”¹:

```python
# åˆ‡æ¢æ¨¡å‹ï¼ˆéœ€è¦é‡æ–°éƒ¨ç½²ï¼‰
VLLM_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # æˆ–å…¶ä»–æ¨¡å‹

# GPUé…ç½®
GPU_COUNT = 1  # A100 GPUæ•°é‡
VLLM_TENSOR_PARALLEL = 1  # Tensorå¹¶è¡Œåº¦

# æ˜¾å­˜é…ç½®
VLLM_GPU_MEMORY_UTILIZATION = 0.90  # æ˜¾å­˜åˆ©ç”¨ç‡

# è¶…æ—¶é…ç½®
CONTAINER_IDLE_TIMEOUT = 30 * 60  # 30åˆ†é’Ÿ
```

### ä½¿ç”¨70Bæ¨¡å‹

å¦‚æœéœ€è¦ä½¿ç”¨70Bæ¨¡å‹ï¼Œä¿®æ”¹é…ç½®å¹¶ä½¿ç”¨2ä¸ªGPU:

```python
VLLM_MODEL = "meta-llama/Llama-3.1-70B-Instruct"
GPU_COUNT = 2
VLLM_TENSOR_PARALLEL = 2
```

ç„¶ååœ¨å‡½æ•°è£…é¥°å™¨ä¸­:
```python
@app.function(
    gpu="A100-80GB:2",  # 2ä¸ªGPU
    ...
)
```

**æ³¨æ„**: 70Bæ¨¡å‹æˆæœ¬çº¦ä¸º8Bæ¨¡å‹çš„2å€ã€‚

## ğŸ’° æˆæœ¬ä¼˜åŒ–

- **8Bæ¨¡å‹**: ~$1.10/å°æ—¶ï¼ˆå•ä¸ªA100-80GBï¼‰
- **70Bæ¨¡å‹**: ~$2.20/å°æ—¶ï¼ˆ2ä¸ªA100-80GBï¼‰
- **è‡ªåŠ¨ä¼‘çœ **: 30åˆ†é’Ÿæ— è¯·æ±‚åè‡ªåŠ¨scaleåˆ°0ï¼Œä¸äº§ç”ŸGPUæˆæœ¬
- **å¿«é€Ÿå†·å¯åŠ¨**: 8Bæ¨¡å‹çº¦1-2åˆ†é’Ÿï¼Œ70Bæ¨¡å‹çº¦3-5åˆ†é’Ÿ

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

æŸ¥çœ‹å®æ—¶æ—¥å¿—:
```bash
modal app logs vllm-integrated
```

æŸ¥çœ‹éƒ¨ç½²çŠ¶æ€:
```bash
modal app list
```

Webç•Œé¢: https://modal.com/apps/ybpang-1/main/deployed/vllm-integrated

## ğŸ” å®‰å…¨é…ç½®

### æ·»åŠ API Keyä¿æŠ¤

åœ¨Modal Secret `vllm-secrets` ä¸­æ·»åŠ :
- `VLLM_WRAPPER_API_KEY`: ä¿æŠ¤WrapperæœåŠ¡

ç„¶ååœ¨è¯·æ±‚æ—¶æ·»åŠ Header:
```bash
curl -H "Authorization: Bearer your-api-key" \
  https://ybpang-1--vllm-integrated-serve.modal.run/chat \
  ...
```

## ğŸ› æ•…éšœæ’æŸ¥

### æœåŠ¡è¿”å›"invalid function call"
- æœåŠ¡æ­£åœ¨å†·å¯åŠ¨ï¼Œç­‰å¾…1-2åˆ†é’Ÿ
- æ£€æŸ¥Modalæ—¥å¿—: `modal app logs vllm-integrated`

### CUDA out of memory
- é™ä½ `VLLM_GPU_MEMORY_UTILIZATION` (å¦‚0.90 â†’ 0.75)
- æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆ70B â†’ 8Bï¼‰
- æˆ–å¢åŠ GPUæ•°é‡å¹¶å¯ç”¨tensor parallelism

### æœåŠ¡è¶…æ—¶
- é¦–æ¬¡è¯·æ±‚éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦5-10åˆ†é’Ÿ
- åç»­è¯·æ±‚ä¼šä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ï¼Œå¯åŠ¨å¿«å¾—å¤š

## ğŸ“ åç»­æ­¥éª¤

1. âœ… æœåŠ¡å·²éƒ¨ç½²å¹¶è¿è¡Œ
2. â³ ç­‰å¾…é¦–æ¬¡æ¨¡å‹ä¸‹è½½å®Œæˆï¼ˆçº¦5-10åˆ†é’Ÿï¼‰
3. ğŸ“ æ›´æ–°åº”ç”¨é…ç½®ä»¥ä½¿ç”¨æ–°çš„URL
4. ğŸ§ª è¿è¡Œæµ‹è¯•ç¡®ä¿é›†æˆæ­£å¸¸
5. ğŸš€ æŠ•å…¥ç”Ÿäº§ä½¿ç”¨

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `modal_vllm_integrated.py` - é›†æˆéƒ¨ç½²é…ç½®
- `modal_vllm.py` - æ—§ç‰ˆvLLMç‹¬ç«‹éƒ¨ç½²ï¼ˆå·²åºŸå¼ƒï¼‰
- `vllm_wrapper.py` - FastAPIåŒ…è£…å±‚ä»£ç ï¼ˆå·²é›†æˆï¼‰
- `modal_vllm_wrapper.py` - Wrapperç‹¬ç«‹éƒ¨ç½²ï¼ˆå·²åºŸå¼ƒï¼‰

---

**éƒ¨ç½²æ—¶é—´**: 2025-12-06
**éƒ¨ç½²è€…**: Claude Code
**çŠ¶æ€**: âœ… å·²éƒ¨ç½²å¹¶è¿è¡Œ
