# ğŸš€ VLLM Auto-Scale æ¶æ„æŒ‡å—

## âœ… éƒ¨ç½²æˆåŠŸ

å·²æˆåŠŸéƒ¨ç½²**åˆ†ç¦»å¼è‡ªåŠ¨ç¼©æ”¾æ¶æ„**ï¼Œå®ç°ï¼š
- âœ… **Wrapperæ°¸è¿œåœ¨çº¿**ï¼Œæ— å†·å¯åŠ¨
- âœ… **GPUè‡ªåŠ¨é‡Šæ”¾**ï¼Œåªåœ¨æ¨ç†æ—¶ä½¿ç”¨
- âœ… **æˆæœ¬æœ€ä¼˜åŒ–**ï¼ŒæŒ‰å®é™…ä½¿ç”¨ä»˜è´¹

---

## ğŸ“ æœåŠ¡ä¿¡æ¯

### WrapperæœåŠ¡ï¼ˆæ°¸è¿œåœ¨çº¿ï¼‰
- **URL**: `https://ybpang-1--vllm-autoscale-wrapper.modal.run`
- **çŠ¶æ€**: å§‹ç»ˆè¿è¡Œ
- **èµ„æº**: æ— GPUï¼Œæˆæœ¬æä½ (~$0.0001/å°æ—¶)
- **å“åº”**: æ¯«ç§’çº§

### vLLMæ¨ç†æœåŠ¡ï¼ˆè‡ªåŠ¨ç¼©æ”¾ï¼‰
- **æ¨¡å‹**: Llama-3.1-8B-Instruct
- **GPU**: 1x A100-80GB
- **è‡ªåŠ¨ç¼©æ”¾**: 2åˆ†é’Ÿæ— è¯·æ±‚åé‡Šæ”¾GPU
- **æˆæœ¬**: ~$1.10/å°æ—¶ï¼ˆä»…åœ¨æ¨ç†æ—¶ï¼‰

---

## ğŸ—ï¸ æ¶æ„è¯´æ˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®¢æˆ·ç«¯è¯·æ±‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTPS
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Wrapper (æ°¸è¿œåœ¨çº¿)         â”‚
â”‚  âœ“ æ— GPUï¼Œæˆæœ¬æä½                  â”‚
â”‚  âœ“ æ¯«ç§’çº§å“åº”                       â”‚
â”‚  âœ“ å¤„ç†HTTPè¯·æ±‚                     â”‚
â”‚  âœ“ å¯æ·»åŠ ç¼“å­˜/é™æµ/æ—¥å¿—              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Modal Function Call
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  vLLMæ¨ç†å‡½æ•° (è‡ªåŠ¨ç¼©æ”¾)            â”‚
â”‚  âœ“ æœ‰GPUï¼ŒæŒ‰éœ€ä½¿ç”¨                  â”‚
â”‚  âœ“ 2åˆ†é’Ÿæ— è¯·æ±‚â†’é‡Šæ”¾GPU              â”‚
â”‚  âœ“ æœ‰è¯·æ±‚æ—¶â†’è‡ªåŠ¨å¯åŠ¨GPU             â”‚
â”‚  âœ“ å¯åŠ¨æ—¶é—´ï¼š1-2åˆ†é’Ÿï¼ˆé¦–æ¬¡5-10åˆ†é’Ÿï¼‰â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å·¥ä½œæµç¨‹

1. **å®¢æˆ·ç«¯å‘é€è¯·æ±‚** â†’ Wrapperï¼ˆç«‹å³å“åº”ï¼‰
2. **Wrapperè°ƒç”¨vLLMå‡½æ•°**:
   - å¦‚æœGPUå·²åŠ è½½ â†’ ç›´æ¥æ¨ç†ï¼ˆç§’çº§ï¼‰
   - å¦‚æœGPUå·²é‡Šæ”¾ â†’ å¯åŠ¨å¹¶åŠ è½½æ¨¡å‹ï¼ˆ1-2åˆ†é’Ÿï¼‰
3. **æ¨ç†å®Œæˆ** â†’ è¿”å›ç»“æœ
4. **2åˆ†é’Ÿæ— æ–°è¯·æ±‚** â†’ GPUè‡ªåŠ¨é‡Šæ”¾

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•Wrapperï¼ˆåº”ç«‹å³å“åº”ï¼‰

```bash
curl https://ybpang-1--vllm-autoscale-wrapper.modal.run/
```

å“åº”ï¼š
```json
{
  "service": "VLLM Auto-Scale Service",
  "version": "2.0.0",
  "architecture": {
    "wrapper": "Always-on (no GPU)",
    "vllm": "Auto-scale to zero (with GPU)",
    "scaledown_window": "2 minutes"
  },
  "model": "meta-llama/Llama-3.1-8B-Instruct"
}
```

### 2. å¥åº·æ£€æŸ¥

```bash
curl https://ybpang-1--vllm-autoscale-wrapper.modal.run/health
```

å“åº”ï¼š
```json
{
  "status": "healthy",
  "wrapper_status": "running",
  "vllm_status": "auto-scaling (idle or active)",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "architecture": "separated"
}
```

### 3. å‘é€å¯¹è¯è¯·æ±‚

**é¦–æ¬¡è¯·æ±‚**ï¼ˆéœ€è¦å¯åŠ¨vLLMï¼Œçº¦1-2åˆ†é’Ÿï¼‰ï¼š
```bash
curl -X POST https://ybpang-1--vllm-autoscale-wrapper.modal.run/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello! Tell me a joke."}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

**åç»­è¯·æ±‚**ï¼ˆ2åˆ†é’Ÿå†…ï¼Œç§’çº§å“åº”ï¼‰ï¼š
```bash
curl -X POST https://ybpang-1--vllm-autoscale-wrapper.modal.run/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "max_tokens": 50
  }'
```

### 4. Pythonå®¢æˆ·ç«¯ç¤ºä¾‹

```python
import httpx
import asyncio

async def chat_with_vllm(message: str):
    """å‘é€å¯¹è¯è¯·æ±‚"""
    async with httpx.AsyncClient(timeout=180.0) as client:  # 3åˆ†é’Ÿè¶…æ—¶ï¼ˆå«å¯åŠ¨æ—¶é—´ï¼‰
        response = await client.post(
            "https://ybpang-1--vllm-autoscale-wrapper.modal.run/chat",
            json={
                "messages": [
                    {"role": "user", "content": message}
                ],
                "max_tokens": 200,
                "temperature": 0.7
            }
        )

        if response.status_code == 200:
            result = response.json()
            print(f"å›å¤: {result['content']}")
            print(f"Tokenä½¿ç”¨: {result['usage']}")
        else:
            print(f"é”™è¯¯: {response.text}")

# ä½¿ç”¨ç¤ºä¾‹
asyncio.run(chat_with_vllm("è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—"))
```

---

## ğŸ’° æˆæœ¬åˆ†æ

### ä¼ ç»Ÿæ–¹æ¡ˆï¼ˆé›†æˆéƒ¨ç½²ï¼‰
- **GPUæˆæœ¬**: $1.10/å°æ—¶
- **è¿è¡Œæ—¶é—´**: 24å°æ—¶ï¼ˆå³ä½¿æ²¡æœ‰è¯·æ±‚ï¼‰
- **æœˆæˆæœ¬**: $1.10 Ã— 24 Ã— 30 = **$792/æœˆ**

### è‡ªåŠ¨ç¼©æ”¾æ–¹æ¡ˆï¼ˆå½“å‰æ¶æ„ï¼‰
- **Wrapperæˆæœ¬**: ~$0.0001/å°æ—¶ Ã— 24 Ã— 30 = **$0.07/æœˆ**
- **GPUæˆæœ¬**: $1.10/å°æ—¶ Ã— å®é™…ä½¿ç”¨å°æ—¶æ•°

**ç¤ºä¾‹è®¡ç®—**ï¼š
- å‡è®¾æ¯å¤©ä½¿ç”¨2å°æ—¶
- æœˆæˆæœ¬ = $0.07ï¼ˆWrapperï¼‰+ $1.10 Ã— 2 Ã— 30ï¼ˆGPUï¼‰= **$66.07/æœˆ**
- **èŠ‚çœ**: 91% ğŸ’°

### æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **ä½é¢‘ä½¿ç”¨** (æ¯å¤©<2å°æ—¶): ä½¿ç”¨è‡ªåŠ¨ç¼©æ”¾
2. **é«˜é¢‘ä½¿ç”¨** (æ¯å¤©>12å°æ—¶): ä½¿ç”¨é›†æˆéƒ¨ç½²
3. **ä¸­ç­‰ä½¿ç”¨** (æ¯å¤©2-12å°æ—¶): ä½¿ç”¨è‡ªåŠ¨ç¼©æ”¾

---

## âš™ï¸ é…ç½®è¯´æ˜

### è°ƒæ•´GPUç¼©æ”¾æ—¶é—´

åœ¨ `modal_vllm_autoscale.py` ä¸­ä¿®æ”¹ï¼š

```python
@app.cls(
    ...
    scaledown_window=120,  # ç§’æ•°ï¼Œ120=2åˆ†é’Ÿ
)
class VLLMInference:
    ...
```

**å»ºè®®å€¼**ï¼š
- **é«˜é¢‘ä½¿ç”¨**: 300-600ç§’ï¼ˆ5-10åˆ†é’Ÿï¼‰- å‡å°‘å†·å¯åŠ¨
- **ä½é¢‘ä½¿ç”¨**: 60-120ç§’ï¼ˆ1-2åˆ†é’Ÿï¼‰- æœ€å¤§åŒ–èŠ‚çœ
- **å¼€å‘æµ‹è¯•**: 30-60ç§’ - å¿«é€Ÿè¿­ä»£

### åˆ‡æ¢æ¨¡å‹

```python
VLLM_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # æˆ–å…¶ä»–æ¨¡å‹
```

æ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹ï¼š
- `meta-llama/Llama-3.1-8B-Instruct` (å½“å‰)
- `meta-llama/Llama-3.1-70B-Instruct` (éœ€è¦2ä¸ªGPU)
- `mistralai/Mistral-7B-Instruct-v0.2`

### ä½¿ç”¨70Bæ¨¡å‹

ä¿®æ”¹ `modal_vllm_autoscale.py`:

```python
VLLM_MODEL = "meta-llama/Llama-3.1-70B-Instruct"

@app.cls(
    gpu="A100-80GB:2",  # 2ä¸ªGPU
    ...
)
class VLLMInference:
    @modal.enter()
    def setup(self):
        self.llm = LLM(
            model=VLLM_MODEL,
            tensor_parallel_size=2,  # ä½¿ç”¨2ä¸ªGPU
            ...
        )
```

**æ³¨æ„**: 70Bæ¨¡å‹æˆæœ¬ â‰ˆ $2.20/å°æ—¶ï¼ˆ2å€ï¼‰

---

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æŸ¥çœ‹å®æ—¶æ—¥å¿—

```bash
# æŸ¥çœ‹æ‰€æœ‰æœåŠ¡æ—¥å¿—
modal app logs vllm-autoscale

# æŸ¥çœ‹ç‰¹å®šå‡½æ•°æ—¥å¿—
modal function logs vllm-autoscale::VLLMInference
```

### æŸ¥çœ‹éƒ¨ç½²çŠ¶æ€

```bash
modal app list
```

### Webç•Œé¢

https://modal.com/apps/ybpang-1/main/deployed/vllm-autoscale

---

## ğŸ§ª æµ‹è¯•è‡ªåŠ¨ç¼©æ”¾

### æµ‹è¯•1: Wrapperæ°¸è¿œåœ¨çº¿

```bash
# åº”è¯¥ç«‹å³å“åº”ï¼ˆæ¯«ç§’çº§ï¼‰
time curl -s https://ybpang-1--vllm-autoscale-wrapper.modal.run/health
```

### æµ‹è¯•2: vLLMå†·å¯åŠ¨

```bash
# é¦–æ¬¡è¯·æ±‚ï¼ˆæˆ–2åˆ†é’Ÿåï¼‰ï¼Œéœ€è¦1-2åˆ†é’Ÿ
time curl -X POST https://ybpang-1--vllm-autoscale-wrapper.modal.run/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hi"}]}'
```

### æµ‹è¯•3: vLLMçƒ­å¯åŠ¨

```bash
# åœ¨2åˆ†é’Ÿå†…å†æ¬¡è¯·æ±‚ï¼Œåº”è¯¥ç§’çº§å“åº”
time curl -X POST https://ybpang-1--vllm-autoscale-wrapper.modal.run/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hi again"}]}'
```

### æµ‹è¯•4: GPUè‡ªåŠ¨é‡Šæ”¾

1. å‘é€ä¸€ä¸ªè¯·æ±‚
2. ç­‰å¾…2åˆ†é’Ÿï¼ˆä¸å‘é€ä»»ä½•è¯·æ±‚ï¼‰
3. åœ¨Modal Webç•Œé¢æŸ¥çœ‹GPUæ˜¯å¦å·²é‡Šæ”¾
4. å†æ¬¡å‘é€è¯·æ±‚ï¼Œè§‚å¯Ÿå†·å¯åŠ¨

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### Wrapperå“åº”æ…¢
- **å¯èƒ½åŸå› **: Modalå¹³å°é—®é¢˜
- **è§£å†³æ–¹æ³•**: æ£€æŸ¥ModalçŠ¶æ€é¡µé¢

### vLLMå¯åŠ¨å¤±è´¥
- **ç—‡çŠ¶**: è¯·æ±‚è¶…æ—¶æˆ–500é”™è¯¯
- **å¯èƒ½åŸå› **:
  - OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰
  - æ¨¡å‹ä¸‹è½½å¤±è´¥
  - Secreté…ç½®é”™è¯¯
- **è§£å†³æ–¹æ³•**:
  ```bash
  modal function logs vllm-autoscale::VLLMInference
  ```

### é¦–æ¬¡è¯·æ±‚è¶…æ—¶
- **åŸå› **: é¦–æ¬¡éœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆ5-10åˆ†é’Ÿï¼‰
- **è§£å†³æ–¹æ³•**:
  - å¢åŠ å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´ï¼ˆå¦‚180ç§’ï¼‰
  - ç­‰å¾…æ¨¡å‹ä¸‹è½½å®Œæˆåé‡è¯•

### GPUä¸€ç›´ä¸é‡Šæ”¾
- **æ£€æŸ¥**: Modal Webç•Œé¢æŸ¥çœ‹æ´»è·ƒå®ä¾‹
- **å¯èƒ½åŸå› **: è¯·æ±‚é—´éš”<2åˆ†é’Ÿ
- **éªŒè¯**: åœæ­¢å‘é€è¯·æ±‚ï¼Œç­‰å¾…2åˆ†é’Ÿåæ£€æŸ¥

---

## ğŸ”„ é‡æ–°éƒ¨ç½²

ä¿®æ”¹é…ç½®åé‡æ–°éƒ¨ç½²ï¼š

```bash
cd /path/to/AiAgentSystem
modal deploy modal_vllm_autoscale.py
```

**æ³¨æ„**:
- Wrapperä¼šç«‹å³æ›´æ–°
- vLLMå®ä¾‹ä¼šåœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶ä½¿ç”¨æ–°é…ç½®

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | é›†æˆéƒ¨ç½² | è‡ªåŠ¨ç¼©æ”¾ |
|------|---------|---------|
| Wrapperå“åº” | å¯åŠ¨åæ¯«ç§’çº§ | å§‹ç»ˆæ¯«ç§’çº§ âœ… |
| é¦–æ¬¡æ¨ç† | 1-2åˆ†é’Ÿ | 1-2åˆ†é’Ÿ |
| çƒ­æ¨ç† | ç§’çº§ | ç§’çº§ |
| GPUé—²ç½®æˆæœ¬ | $1.10/å°æ—¶ | $0 âœ… |
| é€‚ç”¨åœºæ™¯ | é«˜é¢‘è¿ç»­ | ä½é¢‘é—´æ­‡ âœ… |
| æœˆæˆæœ¬ï¼ˆä½é¢‘ï¼‰ | $792 | $66 âœ… |

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### ä½•æ—¶ä½¿ç”¨è‡ªåŠ¨ç¼©æ”¾ï¼ˆæ¨èï¼‰
- âœ… å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒ
- âœ… ä½é¢‘ä½¿ç”¨åœºæ™¯ï¼ˆæ¯å¤©<2å°æ—¶ï¼‰
- âœ… çªå‘æµé‡ï¼ˆä¸å¯é¢„æµ‹ï¼‰
- âœ… æˆæœ¬æ•æ„Ÿé¡¹ç›®

### ä½•æ—¶ä½¿ç”¨é›†æˆéƒ¨ç½²
- âš ï¸ 7Ã—24å°æ—¶è¿ç»­è¿è¡Œ
- âš ï¸ å¯¹å†·å¯åŠ¨å»¶è¿Ÿé›¶å®¹å¿
- âš ï¸ é«˜é¢‘ä½¿ç”¨ï¼ˆæ¯å¤©>12å°æ—¶ï¼‰

### æ··åˆæ–¹æ¡ˆ
- ç”Ÿäº§ç¯å¢ƒï¼šé›†æˆéƒ¨ç½²ï¼ˆä¿è¯æ€§èƒ½ï¼‰
- å¼€å‘ç¯å¢ƒï¼šè‡ªåŠ¨ç¼©æ”¾ï¼ˆèŠ‚çœæˆæœ¬ï¼‰

---

## ğŸ” å®‰å…¨é…ç½®

### æ·»åŠ API Keyä¿æŠ¤

åœ¨Modal Secret `vllm-secrets` ä¸­æ·»åŠ ï¼š
```bash
VLLM_WRAPPER_API_KEY=your-secret-key
```

ç„¶ååœ¨ä»£ç ä¸­æ·»åŠ éªŒè¯ï¼š
```python
@fastapi_app.post("/chat")
async def chat(request: ChatRequest, authorization: str = Header(None)):
    if authorization != f"Bearer {os.getenv('VLLM_WRAPPER_API_KEY')}":
        raise HTTPException(status_code=401, detail="Unauthorized")
    # ... å¤„ç†è¯·æ±‚
```

---

## ğŸ“ è¿ç§»æŒ‡å—

### ä»é›†æˆéƒ¨ç½²è¿ç§»

**ä¹‹å‰çš„URL**:
```
https://ybpang-1--vllm-integrated-serve.modal.run
```

**æ–°çš„URL**:
```
https://ybpang-1--vllm-autoscale-wrapper.modal.run
```

**APIå…¼å®¹æ€§**: å®Œå…¨å…¼å®¹ï¼Œåªéœ€æ›´æ–°URLå³å¯ã€‚

### ä»£ç æ›´æ–°

```python
# ä¹‹å‰
VLLM_URL = "https://ybpang-1--vllm-integrated-serve.modal.run"

# ä¹‹å
VLLM_URL = "https://ybpang-1--vllm-autoscale-wrapper.modal.run"

# å…¶ä»–ä»£ç æ— éœ€ä¿®æ”¹
```

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `modal_vllm_autoscale.py` - è‡ªåŠ¨ç¼©æ”¾éƒ¨ç½²é…ç½® â­
- `modal_vllm_integrated.py` - é›†æˆéƒ¨ç½²é…ç½®
- `VLLM_INTEGRATED_DEPLOYMENT.md` - é›†æˆéƒ¨ç½²æ–‡æ¡£

---

## ğŸ‰ æ€»ç»“

### å·²å®ç°åŠŸèƒ½
âœ… Wrapperæ°¸è¿œåœ¨çº¿ï¼Œæ— å†·å¯åŠ¨
âœ… GPUè‡ªåŠ¨ç¼©æ”¾ï¼ŒæŒ‰éœ€ä½¿ç”¨
âœ… æˆæœ¬æœ€ä¼˜åŒ–ï¼ˆèŠ‚çœ90%+ï¼‰
âœ… å®Œå…¨å…¼å®¹çš„API
âœ… æ˜“äºç›‘æ§å’Œç®¡ç†

### ä¸‹ä¸€æ­¥
1. âœ… æœåŠ¡å·²éƒ¨ç½²å¹¶è¿è¡Œ
2. ğŸ§ª æµ‹è¯•è‡ªåŠ¨ç¼©æ”¾è¡Œä¸º
3. ğŸ“ æ›´æ–°åº”ç”¨é…ç½®ä½¿ç”¨æ–°URL
4. ğŸš€ æŠ•å…¥ç”Ÿäº§ä½¿ç”¨

---

**éƒ¨ç½²æ—¶é—´**: 2025-12-06
**éƒ¨ç½²è€…**: Claude Code
**çŠ¶æ€**: âœ… å·²éƒ¨ç½²ï¼ŒWrapperè¿è¡Œä¸­ï¼ŒvLLMè‡ªåŠ¨ç¼©æ”¾
