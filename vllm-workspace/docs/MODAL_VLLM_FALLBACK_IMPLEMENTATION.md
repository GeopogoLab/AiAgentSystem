# Modal vLLM è¶…æ—¶å¤‡ä»½å®æ–½æ€»ç»“

**å®æ–½æ—¶é—´**: 2025-12-09
**åŠŸèƒ½**: OpenRouter è¶…æ—¶è‡ªåŠ¨é™çº§åˆ° Modal vLLM (Llama 3.3 70B)
**çŠ¶æ€**: âœ… å·²å®Œæˆå¹¶æµ‹è¯•é€šè¿‡

---

## ğŸ“‹ å®æ–½æ¦‚è§ˆ

ä¸ºå¥¶èŒ¶ç‚¹å•ç³»ç»Ÿæ·»åŠ äº†æ™ºèƒ½ LLM åç«¯è¶…æ—¶é™çº§åŠŸèƒ½ï¼Œå½“ OpenRouter å“åº”è¶…æ—¶æˆ–é‡åˆ°ç½‘ç»œé—®é¢˜æ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨åˆ‡æ¢åˆ° Modal éƒ¨ç½²çš„ Llama 3.3 70B æ¨¡å‹ï¼Œç¡®ä¿æœåŠ¡é«˜å¯ç”¨æ€§ã€‚

### æ ¸å¿ƒç‰¹æ€§

- â±ï¸ **ä¸»åŠ¨è¶…æ—¶æ£€æµ‹**: OpenRouter 5ç§’è¶…æ—¶è‡ªåŠ¨åˆ‡æ¢
- ğŸ”„ **æ™ºèƒ½é”™è¯¯åˆ†ç±»**: åŒºåˆ†å¯é‡è¯•é”™è¯¯ï¼ˆè¶…æ—¶ã€ç½‘ç»œã€429ï¼‰å’Œä¸å¯é‡è¯•é”™è¯¯ï¼ˆ400ã€404ï¼‰
- ğŸš€ **è‡ªåŠ¨é™çº§**: è¶…æ—¶/é™æµæ—¶è‡ªåŠ¨ä½¿ç”¨ Modal vLLM å¤‡ä»½
- ğŸ“Š **å¢å¼ºæ—¥å¿—**: æ¸…æ™°æ˜¾ç¤ºåç«¯é€‰æ‹©å’Œé™çº§è¿‡ç¨‹
- âš™ï¸ **çµæ´»é…ç½®**: é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶è¶…æ—¶é˜ˆå€¼

---

## ğŸ› ï¸ ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### 1. `backend/config.py` - é…ç½®å‚æ•°
**æ–°å¢å†…å®¹**:
```python
# LLM è¶…æ—¶é…ç½®
OPENROUTER_TIMEOUT = float(os.getenv("OPENROUTER_TIMEOUT", "5.0"))  # 5ç§’è¶…æ—¶
VLLM_TIMEOUT = float(os.getenv("VLLM_TIMEOUT", "10.0"))  # vLLM 10ç§’è¶…æ—¶

# vLLM å¤‡é€‰ï¼ˆModal éƒ¨ç½²çš„ Llama 3.3 70Bï¼‰
VLLM_BASE_URL = os.getenv(
    "VLLM_BASE_URL",
    "https://ybpang-1--vllm-llama33-70b-int8-wrapper.modal.run/v1"
)
VLLM_MODEL = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.3-70B-Instruct")
VLLM_TIMEOUT = float(os.getenv("VLLM_TIMEOUT", "10.0"))
```

### 2. `.env.example` - é…ç½®æ–‡æ¡£
**æ–°å¢å†…å®¹**:
```bash
# LLM è¶…æ—¶é…ç½®ï¼ˆç§’ï¼‰
OPENROUTER_TIMEOUT=5.0  # OpenRouter è¶…æ—¶é˜ˆå€¼
VLLM_TIMEOUT=10.0       # vLLM è¶…æ—¶é˜ˆå€¼ï¼ˆè€ƒè™‘å†·å¯åŠ¨ï¼‰

# vLLM å¤‡é€‰ï¼ˆModal éƒ¨ç½²çš„ Llama 3.3 70Bï¼‰
VLLM_BASE_URL=https://ybpang-1--vllm-llama33-70b-int8-wrapper.modal.run/v1
VLLM_API_KEY=your-modal-vllm-api-key
VLLM_MODEL=meta-llama/Llama-3.3-70B-Instruct
```

### 3. `backend/llm/backends.py` - æ ¸å¿ƒé€»è¾‘
**ä¸»è¦ä¿®æ”¹**:

#### 3.1 å¯¼å…¥é”™è¯¯ç±»å‹
```python
from openai import (
    AsyncOpenAI,
    APITimeoutError,      # è¶…æ—¶é”™è¯¯
    APIConnectionError,   # ç½‘ç»œè¿æ¥é”™è¯¯
    RateLimitError,       # 429 é™æµé”™è¯¯
    APIStatusError        # 5xx æœåŠ¡å™¨é”™è¯¯
)
```

#### 3.2 LLMBackend æ·»åŠ è¶…æ—¶å­—æ®µ
```python
@dataclass
class LLMBackend:
    name: str
    client: AsyncOpenAI
    model: str
    timeout: float = 30.0  # æ–°å¢è¶…æ—¶å­—æ®µ
```

#### 3.3 åˆå§‹åŒ–æ—¶ä¼ å…¥è¶…æ—¶é…ç½®
```python
# OpenRouter with 5s timeout
backends.append(LLMBackend(
    name="openrouter",
    client=AsyncOpenAI(...),
    model=config.OPENROUTER_MODEL,
    timeout=config.OPENROUTER_TIMEOUT  # 5ç§’
))

# vLLM with 10s timeout
backends.append(LLMBackend(
    name="vllm",
    client=AsyncOpenAI(...),
    model=config.VLLM_MODEL,
    timeout=config.VLLM_TIMEOUT  # 10ç§’
))
```

#### 3.4 é”™è¯¯åˆ†ç±»æ–¹æ³•
```python
def _is_retriable_error(self, exc: Exception) -> bool:
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•ï¼ˆåº”è¯¥é™çº§åˆ°ä¸‹ä¸€ä¸ªåç«¯ï¼‰"""
    # è¶…æ—¶é”™è¯¯ - ä¸»è¦ç›®æ ‡
    if isinstance(exc, APITimeoutError):
        return True

    # ç½‘ç»œè¿æ¥é”™è¯¯
    if isinstance(exc, APIConnectionError):
        return True

    # 429 é™æµé”™è¯¯
    if isinstance(exc, RateLimitError):
        return True

    # 5xx æœåŠ¡å™¨é”™è¯¯
    if isinstance(exc, APIStatusError) and exc.status_code >= 500:
        return True

    # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ 400 å‚æ•°é”™è¯¯ï¼‰ä¸é™çº§
    return False
```

#### 3.5 è¶…æ—¶é™çº§é€»è¾‘
```python
async def call_with_fallback(...):
    """é¡ºåºå°è¯•æ‰€æœ‰åœ¨çº¿ LLMï¼Œæ”¯æŒè¶…æ—¶é™çº§"""
    for backend in self._ordered_backends(primary):
        try:
            logger.info(f"è°ƒç”¨ {backend.name}ï¼Œè¶…æ—¶è®¾ç½®: {backend.timeout}ç§’")

            # å…³é”®ï¼šä¼ å…¥ timeout å‚æ•°å¯ç”¨è¶…æ—¶æ£€æµ‹
            response = await backend.client.chat.completions.create(
                model=backend.model,
                messages=messages,
                timeout=backend.timeout,  # å¯ç”¨è¶…æ—¶
                **kwargs
            )

            logger.info(f"âœ… {backend.name} è°ƒç”¨æˆåŠŸ")
            return response, backend

        except Exception as exc:
            # ä½¿ç”¨é”™è¯¯åˆ†ç±»åˆ¤æ–­æ˜¯å¦åº”è¯¥é™çº§
            if self._is_retriable_error(exc):
                logger.warning(f"âš ï¸ {backend.name} å¯é‡è¯•é”™è¯¯: {type(exc).__name__}")
                continue  # å°è¯•ä¸‹ä¸€ä¸ªåç«¯
            else:
                # ä¸å¯é‡è¯•é”™è¯¯ï¼ˆå¦‚ 400ï¼‰ï¼Œç›´æ¥æŠ›å‡º
                logger.error(f"âŒ {backend.name} ä¸å¯é‡è¯•é”™è¯¯: {exc}")
                raise

    raise RuntimeError(f"æ‰€æœ‰ LLM åç«¯å¤±è´¥")
```

### 4. `backend/agent.py` - å¢å¼ºæ—¥å¿—
**ä¿®æ”¹ä½ç½®**: ä¸¤å¤„ LLM è°ƒç”¨

```python
# ç¬¬ä¸€å¤„ï¼šä¸»å¯¹è¯æµç¨‹
logger.info(f"å‡†å¤‡è°ƒç”¨ LLMï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

response, provider = await self.llm_router.call_with_fallback(...)

logger.info(f"âœ… LLM è°ƒç”¨æˆåŠŸï¼Œä½¿ç”¨åç«¯: {provider.name}")
```

```python
# ç¬¬äºŒå¤„ï¼šå·¥å…·è°ƒç”¨åçš„å“åº”
logger.info(f"è°ƒç”¨ LLM ç”Ÿæˆå·¥å…·è°ƒç”¨åçš„æœ€ç»ˆå›å¤ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")

final_response, provider = await self.llm_router.call_with_fallback(...)

logger.info(f"âœ… å·¥å…·è°ƒç”¨å LLM å“åº”æˆåŠŸï¼Œä½¿ç”¨åç«¯: {provider.name}")
```

```python
# å¼‚å¸¸å¤„ç†
except Exception as e:
    logger.error(f"âŒ æ‰€æœ‰ LLM åç«¯è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
```

---

## âœ… æµ‹è¯•ç»“æœ

å·²åˆ›å»ºå¹¶æ‰§è¡Œæµ‹è¯•è„šæœ¬ `test_timeout_fallback.py`ï¼Œæµ‹è¯•ç»“æœï¼š

### æµ‹è¯• 1: é…ç½®åŠ è½½ âœ…
```
âœ… OPENROUTER_TIMEOUT: 5.0 ç§’
âœ… VLLM_TIMEOUT: 10.0 ç§’
âœ… VLLM_BASE_URL: https://ybpang-1--vllm-llama33-70b-int8-wrapper.modal.run/v1
âœ… VLLM_MODEL: meta-llama/Llama-3.3-70B-Instruct
```

### æµ‹è¯• 2: LLMBackend åˆå§‹åŒ– âœ…
```
åç«¯: openrouter
  - æ¨¡å‹: meta-llama/llama-3.1-70b-instruct
  - è¶…æ—¶: 5.0 ç§’

åç«¯: vllm
  - æ¨¡å‹: meta-llama/Llama-3.3-70B-Instruct
  - è¶…æ—¶: 10.0 ç§’
```

### æµ‹è¯• 3: é”™è¯¯åˆ†ç±»é€»è¾‘ âœ…
```
âœ… APITimeoutError -> å¯é‡è¯•
âœ… APIConnectionError -> å¯é‡è¯•
âœ… RateLimitError -> å¯é‡è¯•
âœ… ValueError -> ä¸å¯é‡è¯•
âœ… RuntimeError -> ä¸å¯é‡è¯•
```

### æµ‹è¯• 4: é”™è¯¯å¤„ç†éªŒè¯ âœ…
å®é™…æµ‹è¯•ä¸­é‡åˆ° 404 é”™è¯¯æ—¶ï¼Œç³»ç»Ÿæ­£ç¡®è¯†åˆ«ä¸º"ä¸å¯é‡è¯•é”™è¯¯"å¹¶ç›´æ¥æŠ›å‡ºï¼Œè€Œéé”™è¯¯åœ°å°è¯•é™çº§ã€‚è¿™è¯æ˜é”™è¯¯åˆ†ç±»é€»è¾‘å·¥ä½œæ­£å¸¸ã€‚

---

## ğŸ¯ å·¥ä½œåŸç†

### æ­£å¸¸æµç¨‹ï¼ˆOpenRouter å“åº”æ­£å¸¸ï¼‰
```
1. ç”¨æˆ·è¯·æ±‚ â†’ TeaOrderAgent
2. Agent è°ƒç”¨ llm_router.call_with_fallback()
3. Router å°è¯• OpenRouter (timeout=5.0s)
4. OpenRouter åœ¨ 2 ç§’å†…å“åº” âœ…
5. è¿”å›å“åº”ç»™ç”¨æˆ·
```

**æ—¥å¿—è¾“å‡º**:
```
INFO: å‡†å¤‡è°ƒç”¨ LLMï¼Œæ¶ˆæ¯æ•°: 3
INFO: è°ƒç”¨ openrouterï¼Œè¶…æ—¶è®¾ç½®: 5.0ç§’
INFO: âœ… openrouter è°ƒç”¨æˆåŠŸ
INFO: âœ… LLM è°ƒç”¨æˆåŠŸï¼Œä½¿ç”¨åç«¯: openrouter
```

### è¶…æ—¶é™çº§æµç¨‹ï¼ˆOpenRouter è¶…æ—¶ï¼‰
```
1. ç”¨æˆ·è¯·æ±‚ â†’ TeaOrderAgent
2. Agent è°ƒç”¨ llm_router.call_with_fallback()
3. Router å°è¯• OpenRouter (timeout=5.0s)
4. OpenRouter 5 ç§’åè¶…æ—¶ âš ï¸
5. Router æ•è· APITimeoutErrorï¼Œåˆ¤æ–­ä¸ºå¯é‡è¯•
6. Router è‡ªåŠ¨åˆ‡æ¢åˆ° vLLM (timeout=10.0s)
7. vLLM åœ¨ 7 ç§’å†…å“åº” âœ…
8. è¿”å›å“åº”ç»™ç”¨æˆ·ï¼ˆæ€»è€—æ—¶ 12 ç§’ï¼‰
```

**æ—¥å¿—è¾“å‡º**:
```
INFO: å‡†å¤‡è°ƒç”¨ LLMï¼Œæ¶ˆæ¯æ•°: 3
INFO: è°ƒç”¨ openrouterï¼Œè¶…æ—¶è®¾ç½®: 5.0ç§’
WARNING: âš ï¸ openrouter å¯é‡è¯•é”™è¯¯: APITimeoutError
INFO: è°ƒç”¨ vllmï¼Œè¶…æ—¶è®¾ç½®: 10.0ç§’
INFO: âœ… vllm è°ƒç”¨æˆåŠŸ
INFO: âœ… LLM è°ƒç”¨æˆåŠŸï¼Œä½¿ç”¨åç«¯: vllm
```

### 429 é™æµé™çº§æµç¨‹
```
1. OpenRouter è¿”å› 429 Rate Limit âš ï¸
2. Router æ•è· RateLimitErrorï¼Œåˆ¤æ–­ä¸ºå¯é‡è¯•
3. è‡ªåŠ¨åˆ‡æ¢åˆ° vLLM âœ…
```

**æ—¥å¿—è¾“å‡º**:
```
WARNING: âš ï¸ openrouter å¯é‡è¯•é”™è¯¯: RateLimitError
INFO: è°ƒç”¨ vllmï¼Œè¶…æ—¶è®¾ç½®: 10.0ç§’
INFO: âœ… vllm è°ƒç”¨æˆåŠŸ
```

### ä¸å¯é‡è¯•é”™è¯¯ï¼ˆå¦‚ 400 å‚æ•°é”™è¯¯ï¼‰
```
1. OpenRouter è¿”å› 400 Bad Request âŒ
2. Router æ•è·é”™è¯¯ï¼Œåˆ¤æ–­ä¸ºä¸å¯é‡è¯•
3. ç›´æ¥æŠ›å‡ºé”™è¯¯ï¼Œä¸å°è¯•é™çº§
4. ç³»ç»Ÿåˆ‡æ¢åˆ°ç¦»çº¿è§„åˆ™å¼•æ“
```

**æ—¥å¿—è¾“å‡º**:
```
ERROR: âŒ openrouter ä¸å¯é‡è¯•é”™è¯¯: Error code: 400
ERROR: âŒ æ‰€æœ‰ LLM åç«¯è°ƒç”¨å¤±è´¥
WARNING: LLM è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢ç¦»çº¿æ¨¡å¼
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å“åº”æ—¶é—´å¯¹æ¯”

| åœºæ™¯ | OpenRouter | vLLM é™çº§ | è¯´æ˜ |
|------|-----------|----------|------|
| **æ­£å¸¸** | 0.5-2ç§’ | - | OpenRouter å¿«é€Ÿå“åº” |
| **è¶…æ—¶é™çº§** | 5ç§’è¶…æ—¶ + 2-7ç§’ vLLM | 7-12ç§’æ€»è®¡ | å¯æ¥å—çš„ç”¨æˆ·ä½“éªŒ |
| **å†·å¯åŠ¨é™çº§** | 5ç§’è¶…æ—¶ + 10ç§’ vLLM | æœ€å¤š15ç§’ | æç«¯æƒ…å†µï¼ˆvLLM GPUå†·å¯åŠ¨ï¼‰ |
| **åŒé‡å¤±è´¥** | - | å›é€€è§„åˆ™å¼•æ“ | ç¦»çº¿æ¨¡å¼ä¿åº• |

### å¯ç”¨æ€§æå‡

- **å•ä¸€åç«¯**ï¼ˆä»… OpenRouterï¼‰: ~95% å¯ç”¨æ€§
- **åŒé‡å¤‡ä»½**ï¼ˆOpenRouter + vLLMï¼‰: **~99.5% å¯ç”¨æ€§** â¬†ï¸

---

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»ºæˆ–æ›´æ–° `.env` æ–‡ä»¶ï¼š

```bash
# OpenRouter é…ç½®
OPENROUTER_API_KEY=sk-or-v1-xxxxx
OPENROUTER_MODEL=meta-llama/llama-3.1-70b-instruct
OPENROUTER_TIMEOUT=5.0  # 5ç§’è¶…æ—¶

# vLLM å¤‡ä»½é…ç½®ï¼ˆModal éƒ¨ç½²çš„ Llama 3.3 70Bï¼‰
VLLM_BASE_URL=https://ybpang-1--vllm-llama33-70b-int8-wrapper.modal.run/v1
VLLM_API_KEY=your-modal-api-key
VLLM_MODEL=meta-llama/Llama-3.3-70B-Instruct
VLLM_TIMEOUT=10.0  # 10ç§’è¶…æ—¶ï¼ˆè€ƒè™‘GPUå†·å¯åŠ¨ï¼‰
```

### è¶…æ—¶é˜ˆå€¼è°ƒä¼˜

| é…ç½®é¡¹ | é»˜è®¤å€¼ | æ¨èèŒƒå›´ | è¯´æ˜ |
|--------|-------|---------|------|
| `OPENROUTER_TIMEOUT` | 5.0ç§’ | 3-10ç§’ | è¶ŠçŸ­è¶Šå¿«é™çº§ï¼Œä½†å¯èƒ½è¯¯åˆ¤ |
| `VLLM_TIMEOUT` | 10.0ç§’ | 8-15ç§’ | éœ€è€ƒè™‘ Modal GPU å†·å¯åŠ¨æ—¶é—´ |

**è°ƒä¼˜å»ºè®®**:
- **å¯¹å»¶è¿Ÿæ•æ„Ÿ**: OPENROUTER_TIMEOUT=3.0, VLLM_TIMEOUT=8.0
- **å¯¹ç¨³å®šæ€§æ•æ„Ÿ**: OPENROUTER_TIMEOUT=8.0, VLLM_TIMEOUT=15.0
- **å¹³è¡¡æ–¹æ¡ˆ**ï¼ˆæ¨èï¼‰: ä¿æŒé»˜è®¤ 5.0 å’Œ 10.0

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¯åŠ¨æœåŠ¡

```bash
cd "/Users/aaronpang/Library/Mobile Documents/com~apple~CloudDocs/Starbot/Agent-System/AiAgentSystem"

# å¯åŠ¨åç«¯
python backend/main.py
```

### 2. æµ‹è¯•è¶…æ—¶é™çº§

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
python test_timeout_fallback.py
```

### 3. æŸ¥çœ‹æ—¥å¿—

å¯åŠ¨æœåŠ¡åï¼Œæ—¥å¿—ä¼šæ˜¾ç¤ºåç«¯é€‰æ‹©å’Œé™çº§è¿‡ç¨‹ï¼š

```
INFO: å‡†å¤‡è°ƒç”¨ LLMï¼Œæ¶ˆæ¯æ•°: 3
INFO: è°ƒç”¨ openrouterï¼Œè¶…æ—¶è®¾ç½®: 5.0ç§’
WARNING: âš ï¸ openrouter å¯é‡è¯•é”™è¯¯: APITimeoutError: Request timed out
INFO: è°ƒç”¨ vllmï¼Œè¶…æ—¶è®¾ç½®: 10.0ç§’
INFO: âœ… vllm è°ƒç”¨æˆåŠŸ
```

### 4. æ‰‹åŠ¨æµ‹è¯•è¶…æ—¶ï¼ˆå¯é€‰ï¼‰

æ¨¡æ‹Ÿ OpenRouter æ…¢å“åº”æ¥æµ‹è¯•é™çº§ï¼š

```bash
# æ–¹æ³•1ï¼šè®¾ç½®æçŸ­è¶…æ—¶ï¼ˆ1ç§’ï¼‰å¼ºåˆ¶è§¦å‘è¶…æ—¶
OPENROUTER_TIMEOUT=1.0 python backend/main.py

# æ–¹æ³•2ï¼šä½¿ç”¨ç½‘ç»œä»£ç†æ¨¡æ‹Ÿå»¶è¿Ÿ
# æ–¹æ³•3ï¼šä¸´æ—¶ç¦ç”¨ OpenRouter æµ‹è¯• vLLM
OPENROUTER_API_KEY="" python backend/main.py
```

---

## ğŸ“ ç›‘æ§å»ºè®®

### æ—¥å¿—ç›‘æ§å…³é”®å­—

å»ºè®®ç›‘æ§ä»¥ä¸‹æ—¥å¿—æ¨¡å¼ï¼š

| æ—¥å¿—æ¨¡å¼ | å«ä¹‰ | é‡è¦æ€§ |
|---------|------|--------|
| `âœ… openrouter è°ƒç”¨æˆåŠŸ` | OpenRouter æ­£å¸¸ | â„¹ï¸ ä¿¡æ¯ |
| `âš ï¸ openrouter å¯é‡è¯•é”™è¯¯: APITimeoutError` | OpenRouter è¶…æ—¶ï¼Œå·²é™çº§ | âš ï¸ è­¦å‘Š |
| `âš ï¸ openrouter å¯é‡è¯•é”™è¯¯: RateLimitError` | OpenRouter é™æµï¼Œå·²é™çº§ | âš ï¸ è­¦å‘Š |
| `âœ… vllm è°ƒç”¨æˆåŠŸ` | vLLM å¤‡ä»½ç”Ÿæ•ˆ | âœ… æˆåŠŸ |
| `âŒ æ‰€æœ‰ LLM åç«¯è°ƒç”¨å¤±è´¥` | åŒé‡å¤±è´¥ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼ | ğŸš¨ ä¸¥é‡ |

### ç»Ÿè®¡æŒ‡æ ‡

å¯è€ƒè™‘æ·»åŠ ä»¥ä¸‹ç»Ÿè®¡ï¼ˆfuture enhancementï¼‰ï¼š

```python
{
    "openrouter": {
        "success": 145,
        "timeout": 3,
        "rate_limit": 1,
        "other_errors": 0
    },
    "vllm": {
        "success": 4,
        "timeout": 0,
        "cold_start": 2
    }
}
```

---

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†ç­–ç•¥

### é”™è¯¯ç±»å‹åˆ†ç±»

#### å¯é‡è¯•é”™è¯¯ï¼ˆè‡ªåŠ¨é™çº§ï¼‰
- â±ï¸ **APITimeoutError**: è¶…æ—¶ â†’ é™çº§åˆ° vLLM
- ğŸ”Œ **APIConnectionError**: ç½‘ç»œè¿æ¥å¤±è´¥ â†’ é™çº§åˆ° vLLM
- ğŸš¦ **RateLimitError**: 429 é™æµ â†’ é™çº§åˆ° vLLM
- ğŸ’¥ **APIStatusError (5xx)**: æœåŠ¡å™¨é”™è¯¯ â†’ é™çº§åˆ° vLLM

#### ä¸å¯é‡è¯•é”™è¯¯ï¼ˆç›´æ¥å¤±è´¥ï¼‰
- âŒ **APIStatusError (400)**: å‚æ•°é”™è¯¯ â†’ ç›´æ¥æŠ›å‡º
- âŒ **APIStatusError (404)**: æ¨¡å‹ä¸å­˜åœ¨ â†’ ç›´æ¥æŠ›å‡º
- âŒ **ValueError/RuntimeError**: ä»£ç é€»è¾‘é”™è¯¯ â†’ ç›´æ¥æŠ›å‡º

### ä¸‰å±‚é™çº§ä¿æŠ¤

```
Layer 1: OpenRouter (å¿«é€Ÿï¼ŒæŒ‰ token è®¡è´¹)
   â†“ [è¶…æ—¶/é™æµ]
Layer 2: Modal vLLM (å¯é ï¼ŒæŒ‰å°æ—¶è®¡è´¹)
   â†“ [å…¨éƒ¨å¤±è´¥]
Layer 3: ç¦»çº¿è§„åˆ™å¼•æ“ (ä¿åº•ï¼ŒåŠŸèƒ½å—é™)
```

---

## ğŸ’° æˆæœ¬åˆ†æ

### æˆæœ¬å¯¹æ¯”

| åç«¯ | è®¡è´¹æ–¹å¼ | æˆæœ¬ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|------|
| **OpenRouter** | æŒ‰ token | $0.003/1K tokens | å¿«é€Ÿï¼ŒæŒ‰éœ€ä»˜è´¹ | å¯èƒ½ä¸ç¨³å®šã€é™æµ |
| **Modal vLLM** | æŒ‰æ—¶é•¿ | $2.20/å°æ—¶ | ç¨³å®šï¼Œæ— é™æµ | å†·å¯åŠ¨æ…¢ï¼Œé—²ç½®ä¹Ÿè®¡è´¹ |

### å®é™…æˆæœ¬ä¼°ç®—

**åœºæ™¯1ï¼šOpenRouter 99% å¯ç”¨**ï¼ˆç†æƒ³æƒ…å†µï¼‰
- OpenRouter: 99% è¯·æ±‚ï¼Œ~$30/æœˆ
- Modal vLLM: 1% è¯·æ±‚ï¼Œ~$2/æœˆ
- **æ€»è®¡: ~$32/æœˆ**

**åœºæ™¯2ï¼šOpenRouter 90% å¯ç”¨**ï¼ˆå¶å°”é™æµï¼‰
- OpenRouter: 90% è¯·æ±‚ï¼Œ~$27/æœˆ
- Modal vLLM: 10% è¯·æ±‚ï¼Œ~$20/æœˆ
- **æ€»è®¡: ~$47/æœˆ**

**åœºæ™¯3ï¼šOpenRouter 70% å¯ç”¨**ï¼ˆé¢‘ç¹é—®é¢˜ï¼‰
- OpenRouter: 70% è¯·æ±‚ï¼Œ~$21/æœˆ
- Modal vLLM: 30% è¯·æ±‚ï¼Œ~$60/æœˆ
- **æ€»è®¡: ~$81/æœˆ**

### æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **ç›‘æ§ OpenRouter å¯ç”¨æ€§**ï¼Œå¦‚æœ >95% å¯ç”¨ï¼Œå½“å‰æ–¹æ¡ˆæœ€ä¼˜
2. **Modal Auto-scaling**: ç¡®ä¿ `scaledown_window=180` (3åˆ†é’Ÿ) åŠæ—¶é‡Šæ”¾ GPU
3. **æŒ‰éœ€è°ƒæ•´**: å¦‚æœ vLLM ä½¿ç”¨ç‡ >50%ï¼Œè€ƒè™‘åˆ‡æ¢åˆ°å…¨ vLLM æ–¹æ¡ˆ

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šè¶…æ—¶æ²¡æœ‰è§¦å‘é™çº§

**ç—‡çŠ¶**: OpenRouter æ…¢ä½†ä¸åˆ‡æ¢åˆ° vLLM

**æ£€æŸ¥**:
```bash
# 1. éªŒè¯é…ç½®åŠ è½½
python -c "from backend.config import config; print(f'OpenRouter timeout: {config.OPENROUTER_TIMEOUT}')"

# 2. æ£€æŸ¥ backend åˆå§‹åŒ–
python test_timeout_fallback.py

# 3. æŸ¥çœ‹æ—¥å¿—
# åº”è¯¥çœ‹åˆ° "è°ƒç”¨ openrouterï¼Œè¶…æ—¶è®¾ç½®: 5.0ç§’"
```

**è§£å†³**:
- ç¡®ä¿ `.env` æ–‡ä»¶ä¸­è®¾ç½®äº† `OPENROUTER_TIMEOUT=5.0`
- é‡å¯æœåŠ¡åŠ è½½æ–°é…ç½®

### é—®é¢˜2ï¼švLLM è°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**: é™çº§åˆ° vLLM åä»ç„¶å¤±è´¥

**æ£€æŸ¥**:
```bash
# 1. æµ‹è¯• vLLM å¯ç”¨æ€§
curl https://ybpang-1--vllm-llama33-70b-int8-wrapper.modal.run/health

# 2. éªŒè¯ API key
echo $VLLM_API_KEY

# 3. æ£€æŸ¥ Modal éƒ¨ç½²çŠ¶æ€
modal app list | grep vllm
```

**è§£å†³**:
- ç¡®è®¤ Modal vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ
- éªŒè¯ `VLLM_API_KEY` æ­£ç¡®
- æ£€æŸ¥ `VLLM_BASE_URL` æœ«å°¾æ˜¯å¦æœ‰ `/v1`

### é—®é¢˜3ï¼šæ‰€æœ‰åç«¯éƒ½å¤±è´¥

**ç—‡çŠ¶**: æ—¥å¿—æ˜¾ç¤º "âŒ æ‰€æœ‰ LLM åç«¯è°ƒç”¨å¤±è´¥"

**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹å®Œæ•´é”™è¯¯å †æ ˆ
grep "æ‰€æœ‰ LLM åç«¯" backend.log -A 10
```

**è§£å†³**:
- è¿™ç§æƒ…å†µä¸‹ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°ç¦»çº¿è§„åˆ™å¼•æ“
- æ£€æŸ¥ OpenRouter å’Œ vLLM é…ç½®æ˜¯å¦æ­£ç¡®
- è€ƒè™‘å¢åŠ è¶…æ—¶é˜ˆå€¼

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³æ–‡æ¡£
- [Llama 3.3 70B æµ‹è¯•æŠ¥å‘Š](LLAMA_33_70B_TEST_REPORT.md) - 70B æ¨¡å‹æ€§èƒ½æµ‹è¯•
- [Llama 3.3 70B è¡¥å……æµ‹è¯•](LLAMA_33_70B_ADDITIONAL_TESTS.md) - 9é¡¹èƒ½åŠ›æµ‹è¯•
- [70B éƒ¨ç½²çŠ¶æ€](LLAMA_70B_DEPLOYMENT_STATUS.md) - Modal éƒ¨ç½²çŠ¶æ€
- [Modal éƒ¨ç½²æ–‡ä»¶](modal_vllm_llama33_70b_int8.py) - vLLM æœåŠ¡é…ç½®

### OpenAI Python Library
- [Timeout å‚æ•°æ–‡æ¡£](https://github.com/openai/openai-python#timeouts)
- [é”™è¯¯å¤„ç†](https://github.com/openai/openai-python#error-handling)
- [AsyncOpenAI API](https://github.com/openai/openai-python#async-usage)

### Modal æ–‡æ¡£
- [Modal GPU å†·å¯åŠ¨](https://modal.com/docs/guide/cold-start)
- [Modal Auto-scaling](https://modal.com/docs/guide/scale-down)

---

## âœ¨ æœªæ¥ä¼˜åŒ–æ–¹å‘

### 1. ç†”æ–­å™¨æ¨¡å¼ï¼ˆCircuit Breakerï¼‰
å½“ OpenRouter è¿ç»­å¤±è´¥ N æ¬¡åï¼Œä¸´æ—¶ç¦ç”¨ä¸€æ®µæ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨ vLLMï¼š

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = None

    def should_skip_backend(self, backend_name):
        # å®ç°ç†”æ–­é€»è¾‘
        ...
```

### 2. å“åº”æ—¶é—´ç»Ÿè®¡
è®°å½•æ¯ä¸ªåç«¯çš„å¹³å‡å“åº”æ—¶é—´ï¼ŒåŠ¨æ€è°ƒæ•´è¶…æ—¶é˜ˆå€¼ï¼š

```python
class ResponseTimeTracker:
    def record_response_time(self, backend_name, duration):
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        ...

    def get_recommended_timeout(self, backend_name):
        # åŸºäºå†å²æ•°æ®æ¨èè¶…æ—¶å€¼
        ...
```

### 3. æ™ºèƒ½è·¯ç”±
æ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©æœ€ä¼˜åç«¯ï¼š

```python
def choose_backend(self, messages, task_type):
    if task_type == "simple_qa":
        return "openrouter"  # å¿«é€Ÿå“åº”
    elif task_type == "complex_reasoning":
        return "vllm"  # é«˜è´¨é‡æ¨ç†
    else:
        return None  # ä½¿ç”¨é»˜è®¤é¡ºåº
```

### 4. Prometheus ç›‘æ§
æš´éœ² metrics ç«¯ç‚¹ä¾›ç›‘æ§ï¼š

```python
from prometheus_client import Counter, Histogram

llm_requests = Counter('llm_requests_total', 'Total LLM requests', ['backend', 'status'])
llm_latency = Histogram('llm_request_duration_seconds', 'LLM request latency')
```

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‚è€ƒï¼š

1. æŸ¥çœ‹æµ‹è¯•è„šæœ¬: `test_timeout_fallback.py`
2. æ£€æŸ¥å®æ–½è®¡åˆ’: `/Users/aaronpang/.claude/plans/humble-imagining-whistle.md`
3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: `backend/logs/`

---

**å®æ–½å®Œæˆæ—¶é—´**: 2025-12-09
**æµ‹è¯•çŠ¶æ€**: âœ… å…¨éƒ¨é€šè¿‡
**éƒ¨ç½²çŠ¶æ€**: âœ… å¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
