"""
Modal éƒ¨ç½²ï¼šLlama 3.3 70B (BF16/FP16) + Tensor Parallelism

ä½¿ç”¨Tensor Parallelismï¼Œ2ä¸ªA100-80GBè¿è¡Œ
æ˜¾å­˜éœ€æ±‚ï¼šæ¯ä¸ªGPU ~70GBï¼ˆæ€»å…±140GBåˆ†å¸ƒå¼ï¼‰
æ€§èƒ½æŸå¤±ï¼š0%ï¼ˆåŸå§‹ç²¾åº¦ï¼‰

é…ç½®ï¼š
- 2ä¸ªA100-80GB GPU (tensor_parallel_size=2)
- max_model_len=4096 (å……è¶³çš„contextçª—å£)
- gpu_memory_utilization=0.90 (æ¯ä¸ªGPUåˆ†æ‹…ä¸€åŠè´Ÿè½½)
- dtype=auto (BF16æˆ–FP16)
"""
import os
import modal
from typing import List, Dict, Any, Optional

# ===== é…ç½® =====
VLLM_MODEL = os.environ.get("VLLM_MODEL", "meta-llama/Llama-3.3-70B-Instruct")
VLLM_MAX_MODEL_LEN = 4096  # ä½¿ç”¨2ä¸ªGPUåå¯ä»¥æé«˜context
VLLM_GPU_MEMORY_UTILIZATION = 0.90  # 2ä¸ªGPUåˆ†æ‹…è´Ÿè½½

# ===== Modal é•œåƒ =====
vllm_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git")
    .pip_install(
        "vllm==0.6.6.post1",
        "torch==2.5.1",
        "transformers==4.46.0",
        "hf-transfer",
        "bitsandbytes>=0.41.0",  # INT8é‡åŒ–æ”¯æŒ
    )
)

wrapper_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "fastapi>=0.109.0",
        "uvicorn[standard]>=0.27.0",
        "pydantic>=2.9",
    )
)

weights_volume = modal.Volume.from_name("vllm-llama33-70b-int8", create_if_missing=True)

app = modal.App("vllm-llama33-70b-int8")

# ===== vLLM æ¨ç†å‡½æ•° =====
vllm_llm = None


@app.function(
    image=vllm_image,
    # ä½¿ç”¨2ä¸ªGPUè¿›è¡Œtensor parallelism
    gpu="A100-80GB:2",  # 2ä¸ªA100-80GB GPU
    volumes={"/weights": weights_volume},
    secrets=[modal.Secret.from_name("vllm-secrets")],
    scaledown_window=180,  # 3åˆ†é’Ÿåé‡Šæ”¾GPU
    timeout=900,  # 15åˆ†é’Ÿè¶…æ—¶ï¼ˆé¦–æ¬¡ä¸‹è½½æ¨¡å‹è¾ƒå¤§ï¼‰
)
def generate_text_70b_int8(
    messages: List[Dict[str, str]],
    max_tokens: int = 2048,
    temperature: float = 0.7,
    top_p: float = 0.9,
) -> Dict[str, Any]:
    """
    Llama 3.3 70B æ¨ç† (Tensor Parallelism)

    Tensor Parallelismé…ç½®ï¼š
    - ç²¾åº¦ï¼šBF16/FP16ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰
    - æ¨¡å‹æƒé‡ï¼š~140GBï¼ˆåˆ†å¸ƒå¼ï¼‰
    - Tensor Parallelism: æ¨¡å‹åˆ†å¸ƒåˆ°2ä¸ªGPU
    - æ¯ä¸ªGPU: ~70GBæ¨¡å‹ + ~10-15GB KVç¼“å­˜ = ~75-85GB
    - æ€»éœ€æ±‚ï¼š2Ã—A100-80GB = 160GBæ€»æ˜¾å­˜

    GPUé…ç½®ï¼š
    - 2ä¸ªA100-80GB (tensor_parallel_size=2)
    - æ¯ä¸ªGPUè´Ÿè½½~80-90%
    - æ”¯æŒ4K contextçª—å£
    """
    global vllm_llm

    if vllm_llm is None:
        from vllm import LLM, SamplingParams
        import torch

        print(f"ğŸš€ Loading Llama 3.3 70B with Tensor Parallelism")
        print(f"ğŸ“¦ Model: {VLLM_MODEL}")
        print(f"ğŸ’¾ Precision: BF16/FP16 (auto)")
        print(f"ğŸ® GPUs: 2Ã—A100-80GB (tensor_parallel_size=2)")
        print(f"ğŸ“Š Expected memory per GPU: ~70-85GB")
        print(f"ğŸ¯ Max context: {VLLM_MAX_MODEL_LEN} tokens")
        print(f"âš™ï¸  GPU memory util: {VLLM_GPU_MEMORY_UTILIZATION}")

        # æ£€æŸ¥GPUä¿¡æ¯
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"ğŸ® GPU: {gpu_name}")
            print(f"ğŸ’¾ Total VRAM: {total_memory:.1f}GB")

        vllm_llm = LLM(
            model=VLLM_MODEL,
            download_dir="/weights",
            # ä¸ä½¿ç”¨é‡åŒ–ï¼Œä¾èµ–tensor parallelismåˆ†å¸ƒæ¨¡å‹
            # quantization="fp8",  # æš‚æ—¶ç¦ç”¨ï¼Œä¸tensor parallelismå¯èƒ½æœ‰å†²çª
            gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,
            max_model_len=VLLM_MAX_MODEL_LEN,
            tensor_parallel_size=2,  # 2ä¸ªGPUè¿›è¡Œtensor parallelism
            dtype="auto",  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦ï¼ˆé€šå¸¸æ˜¯BF16æˆ–FP16ï¼‰
            # é¢å¤–ä¼˜åŒ–é€‰é¡¹
            enforce_eager=False,  # ä½¿ç”¨CUDA graphsåŠ é€Ÿ
        )

        # æ˜¾ç¤ºå®é™…æ˜¾å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1e9
            reserved = torch.cuda.memory_reserved(0) / 1e9
            print(f"âœ… Model loaded!")
            print(f"ğŸ’¾ Memory allocated: {allocated:.1f}GB")
            print(f"ğŸ’¾ Memory reserved: {reserved:.1f}GB")

    # æ„å»ºpromptï¼ˆLlama 3.3æ ¼å¼ï¼‰
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]

        if role == "system":
            prompt += f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>"
        elif role == "user":
            prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>"
        elif role == "assistant":
            prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>"

    prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"

    # æ¨ç†
    from vllm import SamplingParams

    sampling_params = SamplingParams(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )

    outputs = vllm_llm.generate([prompt], sampling_params)
    output = outputs[0]

    return {
        "text": output.outputs[0].text,
        "prompt_tokens": len(output.prompt_token_ids),
        "completion_tokens": len(output.outputs[0].token_ids),
        "finish_reason": output.outputs[0].finish_reason,
    }


# ===== FastAPI Wrapper =====
@app.function(image=wrapper_image)
@modal.asgi_app()
def wrapper():
    """FastAPI Wrapper - æ°¸è¿œåœ¨çº¿"""
    from fastapi import FastAPI, HTTPException, status
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, Field
    from typing import List, Dict, Optional
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    fastapi_app = FastAPI(
        title="Llama 3.3 70B INT8 Service",
        description="Meta Llama 3.3 70B Instruct with INT8 quantization",
        version="1.0.0",
    )

    fastapi_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # è¯·æ±‚/å“åº”æ¨¡å‹
    class Message(BaseModel):
        role: str
        content: str

    class ChatRequest(BaseModel):
        messages: List[Message]
        max_tokens: Optional[int] = 2048
        temperature: Optional[float] = 0.7
        top_p: Optional[float] = 0.9

    class ChatResponse(BaseModel):
        content: str
        model: str
        usage: Dict[str, int]
        finish_reason: Optional[str] = None

    class HealthResponse(BaseModel):
        status: str
        model: str
        model_version: str
        quantization: str
        gpu_requirement: str
        context_length: int

    # APIç«¯ç‚¹
    @fastapi_app.get("/")
    async def root():
        return {
            "service": "Llama 3.3 70B Service",
            "version": "1.0.0",
            "model": VLLM_MODEL,
            "model_version": "Llama 3.3 (Meta's latest)",
            "precision": "BF16/FP16 (auto)",
            "gpu": "2Ã—A100-80GB (Tensor Parallelism)",
            "memory_per_gpu": "~70-85GB",
            "context_length": VLLM_MAX_MODEL_LEN,
            "quality_loss": "0% (original precision)",
            "endpoints": {
                "POST /chat": "å¯¹è¯æ¥å£",
                "POST /v1/chat/completions": "OpenAIå…¼å®¹æ¥å£",
                "GET /health": "å¥åº·æ£€æŸ¥",
            }
        }

    @fastapi_app.get("/health", response_model=HealthResponse)
    async def health():
        return HealthResponse(
            status="healthy",
            model=VLLM_MODEL,
            model_version="Llama 3.3",
            quantization="None (BF16/FP16 + Tensor Parallelism)",
            gpu_requirement="2Ã—A100-80GB",
            context_length=VLLM_MAX_MODEL_LEN,
        )

    @fastapi_app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """å¯¹è¯æ¥å£"""
        try:
            logger.info(f"æ”¶åˆ°Llama 3.3 70Bè¯·æ±‚ï¼Œ{len(request.messages)}æ¡æ¶ˆæ¯")

            messages_dict = [{"role": m.role, "content": m.content} for m in request.messages]

            logger.info("è°ƒç”¨Llama 3.3 70B INT8æ¨ç†...")

            result = generate_text_70b_int8.remote(
                messages=messages_dict,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
            )

            logger.info("æ¨ç†å®Œæˆ")

            return ChatResponse(
                content=result["text"],
                model=VLLM_MODEL,
                usage={
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                },
                finish_reason=result["finish_reason"],
            )

        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    @fastapi_app.post("/v1/chat/completions")
    async def openai_chat(request: Dict):
        """OpenAIå…¼å®¹æ¥å£"""
        try:
            messages = request.get("messages", [])
            max_tokens = request.get("max_tokens", 2048)
            temperature = request.get("temperature", 0.7)
            top_p = request.get("top_p", 0.9)

            result = generate_text_70b_int8.remote(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )

            return {
                "id": "chatcmpl-llama33-70b",
                "object": "chat.completion",
                "created": int(__import__("time").time()),
                "model": VLLM_MODEL,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["text"],
                    },
                    "finish_reason": result["finish_reason"],
                }],
                "usage": {
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                }
            }

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    return fastapi_app


@app.local_entrypoint()
def main():
    print("=" * 70)
    print("ğŸš€ Llama 3.3 70B (BF16/FP16) + Tensor Parallelism éƒ¨ç½²")
    print("=" * 70)
    print()
    print("ğŸ“¦ æ¨¡å‹é…ç½®:")
    print(f"   - æ¨¡å‹: {VLLM_MODEL}")
    print(f"   - ç‰ˆæœ¬: Llama 3.3 (Metaæœ€æ–°)")
    print(f"   - ç²¾åº¦: BF16/FP16 (autoï¼ŒåŸå§‹ç²¾åº¦)")
    print(f"   - GPU: 2Ã—A100-80GB (Tensor Parallelism)")
    print(f"   - æ¯GPUæ˜¾å­˜: ~70-85GB")
    print(f"   - æ€»æ˜¾å­˜: ~140-170GB (åˆ†å¸ƒå¼)")
    print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {VLLM_MAX_MODEL_LEN} tokens")
    print()
    print("âš¡ æ€§èƒ½ç‰¹ç‚¹:")
    print("   - è´¨é‡æŸå¤±: 0%ï¼ˆåŸå§‹ç²¾åº¦ï¼Œæ— é‡åŒ–ï¼‰")
    print("   - æ¨ç†é€Ÿåº¦: ~40-50 tokens/ç§’")
    print("   - ç²¾åº¦: æœ€é«˜ï¼ˆBF16/FP16ï¼‰")
    print("   - å¯é æ€§: é«˜ï¼ˆtensor parallelismï¼‰")
    print()
    print("ğŸ® GPUé…ç½®:")
    print("   - æ¶æ„: Tensor Parallelism (æ¨¡å‹åˆ‡åˆ†)")
    print("   - GPUs: 2Ã—A100-80GB")
    print("   - æ¯GPUè´Ÿè½½: ~80-90%")
    print("   - tensor_parallel_size: 2")
    print()
    print("ğŸ”§ éƒ¨ç½²å‘½ä»¤:")
    print("   modal deploy modal_vllm_llama33_70b_int8.py")
    print()
    print("ğŸ“ æ³¨æ„äº‹é¡¹:")
    print("   - é¦–æ¬¡ä¸‹è½½Llama 3.3éœ€è¦15-20åˆ†é’Ÿï¼ˆæ¨¡å‹è¾ƒå¤§ï¼‰")
    print("   - éœ€è¦HuggingFaceè®¿é—®æƒé™ï¼ˆMetaæ¨¡å‹éœ€ç”³è¯·ï¼‰")
    print("   - ä½¿ç”¨åŸå§‹BF16/FP16ç²¾åº¦ï¼ˆæ— é‡åŒ–ï¼‰")
    print("   - tensor_parallel_size=2 ç¡®ä¿æ¨¡å‹åˆ†å¸ƒåˆ°2ä¸ªGPU")
    print()
    print("ğŸ’° æˆæœ¬:")
    print("   - 2Ã—A100-80GB: ~$2.20/å°æ—¶")
    print("   - ä¼˜åŠ¿: åŸå§‹ç²¾åº¦ + Tensor Parallelism + é«˜å¯é æ€§")
    print("=" * 70)
