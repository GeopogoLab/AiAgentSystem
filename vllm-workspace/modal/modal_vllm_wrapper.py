"""
VLLM Wrapper FastAPI æœåŠ¡çš„ Modal éƒ¨ç½²é…ç½®

æä¾›ä¸€ä¸ªåŒ…è£…å±‚ï¼Œç®€åŒ–VLLMçš„è°ƒç”¨æ¥å£
å¯ä»¥éƒ¨ç½²åœ¨Modalä¸Šä½œä¸ºç‹¬ç«‹æœåŠ¡
"""
import modal

# Modal é•œåƒé…ç½®
vllm_wrapper_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "fastapi==0.109.0",
        "uvicorn[standard]==0.27.0",
        "pydantic==2.5.3",
        "openai==1.54.0",
        "httpx==0.27.0",
    )
)

# Modal App
app = modal.App("vllm-wrapper")


@app.function(
    image=vllm_wrapper_image,
    secrets=[modal.Secret.from_name("vllm-secrets")],
    timeout=24 * 3600,
    container_idle_timeout=15 * 60,
)
@modal.asgi_app()
def fastapi_app():
    """éƒ¨ç½²VLLM Wrapper FastAPIæœåŠ¡"""
    import os

    # ä» Modal Secret è¯»å–é…ç½®
    os.environ.setdefault("VLLM_BASE_URL", os.getenv("VLLM_BASE_URL", ""))
    os.environ.setdefault("VLLM_MODEL", os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-70B-Instruct"))
    os.environ.setdefault("VLLM_API_KEY", os.getenv("VLLM_API_KEY", ""))
    os.environ.setdefault("VLLM_WRAPPER_API_KEY", os.getenv("VLLM_WRAPPER_API_KEY", ""))

    # å¯¼å…¥åº”ç”¨
    from vllm_wrapper import app as fastapi_application

    return fastapi_application


@app.local_entrypoint()
def main():
    """æœ¬åœ°å…¥å£æç¤º"""
    print("ğŸš€ VLLM Wrapper - Modal éƒ¨ç½²")
    print("=" * 50)
    print("éƒ¨ç½²å‘½ä»¤: modal deploy modal_vllm_wrapper.py")
    print("")
    print("éœ€è¦åœ¨ Modal Secret 'vllm-secrets' ä¸­é…ç½®:")
    print("  - VLLM_BASE_URL: VLLMæœåŠ¡åœ°å€")
    print("  - VLLM_MODEL: æ¨¡å‹åç§°")
    print("  - VLLM_API_KEY: VLLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰")
    print("  - VLLM_WRAPPER_API_KEY: åŒ…è£…æœåŠ¡APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰")
    print("")
    print("éƒ¨ç½²åå¯ä»¥é€šè¿‡ä»¥ä¸‹ç«¯ç‚¹è®¿é—®:")
    print("  - POST /chat - ç®€åŒ–å¯¹è¯æ¥å£")
    print("  - POST /v1/chat/completions - OpenAIå…¼å®¹æ¥å£")
    print("  - GET /health - å¥åº·æ£€æŸ¥")
    print("  - GET /models - åˆ—å‡ºæ¨¡å‹")
