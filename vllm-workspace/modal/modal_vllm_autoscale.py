"""
Modal éƒ¨ç½²ï¼šåˆ†ç¦»æ¶æ„ - Wrapperæ°¸è¿œåœ¨çº¿ï¼ŒvLLMè‡ªåŠ¨ç¼©æ”¾

æ¶æ„ï¼š
1. FastAPI Wrapper (æ— GPUï¼Œæ°¸è¿œåœ¨çº¿) - å¤„ç†HTTPè¯·æ±‚
2. vLLMæ¨ç†å‡½æ•° (æœ‰GPUï¼Œè‡ªåŠ¨ç¼©æ”¾åˆ°0) - åªåœ¨æ¨ç†æ—¶ä½¿ç”¨GPU

ä¼˜åŠ¿ï¼š
- Wrapperæ°¸è¿œåœ¨çº¿ï¼Œæ— å†·å¯åŠ¨
- vLLMä¸ç”¨æ—¶è‡ªåŠ¨é‡Šæ”¾GPUï¼ŒèŠ‚çœæˆæœ¬
- å¯ä»¥åœ¨wrapperå±‚æ·»åŠ ç¼“å­˜ã€é™æµç­‰åŠŸèƒ½
"""
import os
import modal
from typing import List, Dict, Any, Optional

# ===== é…ç½® =====
VLLM_MODEL = os.environ.get("VLLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
VLLM_MAX_MODEL_LEN = 8192
VLLM_GPU_MEMORY_UTILIZATION = 0.90

# ===== Modal é•œåƒ =====
# vLLMé•œåƒï¼ˆéœ€è¦GPUï¼‰
vllm_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git")
    .pip_install(
        "vllm==0.6.6.post1",
        "torch==2.5.1",
        "transformers==4.46.0",
        "hf-transfer",
    )
)

# Wrapperé•œåƒï¼ˆè½»é‡çº§ï¼Œæ— GPUï¼‰
wrapper_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "fastapi>=0.109.0",
        "uvicorn[standard]>=0.27.0",
        "pydantic>=2.9",
        "httpx>=0.27.0",
    )
)

# æ¨¡å‹ç¼“å­˜
weights_volume = modal.Volume.from_name("vllm-llama-cache", create_if_missing=True)

# Modal App
app = modal.App("vllm-autoscale")


# ===== vLLMæ¨ç†å‡½æ•°ï¼ˆè‡ªåŠ¨ç¼©æ”¾ï¼‰ =====
@app.cls(
    image=vllm_image,
    gpu="A100-80GB",
    volumes={"/weights": weights_volume},
    secrets=[modal.Secret.from_name("vllm-secrets")],
    scaledown_window=120,  # 2åˆ†é’Ÿæ— è¯·æ±‚åé‡Šæ”¾GPU
)
class VLLMInference:
    """vLLMæ¨ç†æœåŠ¡ - è‡ªåŠ¨ç¼©æ”¾åˆ°0"""

    @modal.enter()
    def setup(self):
        """åˆå§‹åŒ–vLLMå¼•æ“"""
        import torch
        from vllm import LLM, SamplingParams

        print(f"ğŸš€ Loading model: {VLLM_MODEL}")

        self.llm = LLM(
            model=VLLM_MODEL,
            download_dir="/weights",
            gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,
            max_model_len=VLLM_MAX_MODEL_LEN,
            tensor_parallel_size=1,
        )

        print(f"âœ… Model loaded: {VLLM_MODEL}")

    @modal.method()
    def generate(
        self,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·

        Returns:
            ç”Ÿæˆç»“æœ
        """
        from vllm import SamplingParams

        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
        )

        outputs = self.llm.generate([prompt], sampling_params)
        output = outputs[0]

        return {
            "text": output.outputs[0].text,
            "prompt_tokens": len(output.prompt_token_ids),
            "completion_tokens": len(output.outputs[0].token_ids),
            "finish_reason": output.outputs[0].finish_reason,
        }

    @modal.method()
    def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> Dict[str, Any]:
        """
        å¯¹è¯æ¥å£

        Args:
            messages: å¯¹è¯å†å²
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·

        Returns:
            å¯¹è¯ç»“æœ
        """
        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºprompt
        prompt = self._messages_to_prompt(messages)
        return self.generate(prompt, max_tokens, temperature, top_p)

    def _messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """å°†å¯¹è¯æ¶ˆæ¯è½¬æ¢ä¸ºprompt"""
        prompt = ""
        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "system":
                prompt += f"<|system|>\n{content}\n"
            elif role == "user":
                prompt += f"<|user|>\n{content}\n"
            elif role == "assistant":
                prompt += f"<|assistant|>\n{content}\n"

        prompt += "<|assistant|>\n"
        return prompt


# ===== FastAPI Wrapperï¼ˆæ°¸è¿œåœ¨çº¿ï¼‰ =====
@app.function(
    image=wrapper_image,
    # ä¸è®¾ç½®scaledown_windowï¼Œè®©wrapperä¿æŒè¿è¡Œ
    # WebæœåŠ¡é»˜è®¤è¡Œä¸ºæ˜¯ä¿æŒè‡³å°‘ä¸€ä¸ªå®ä¾‹è¿è¡Œ
    secrets=[modal.Secret.from_name("vllm-secrets")],
)
@modal.asgi_app()
def wrapper():
    """FastAPI Wrapper - æ°¸è¿œåœ¨çº¿ï¼Œè°ƒç”¨vLLMæ¨ç†å‡½æ•°"""
    from fastapi import FastAPI, HTTPException, status
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, Field
    from typing import List, Dict, Optional
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # è·å–vLLMæ¨ç†ç±»çš„å¼•ç”¨ï¼ˆåœ¨åº”ç”¨å¤–éƒ¨ï¼‰
    inference_cls = VLLMInference()

    # FastAPIåº”ç”¨
    fastapi_app = FastAPI(
        title="VLLM Auto-Scale Service",
        description="FastAPI wrapper (æ°¸è¿œåœ¨çº¿) + vLLM (è‡ªåŠ¨ç¼©æ”¾)",
        version="2.0.0",
    )

    fastapi_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # è¯·æ±‚æ¨¡å‹
    class Message(BaseModel):
        role: str
        content: str

    class ChatRequest(BaseModel):
        messages: List[Message]
        max_tokens: Optional[int] = Field(2048, description="æœ€å¤§tokenæ•°")
        temperature: Optional[float] = Field(0.7, description="æ¸©åº¦å‚æ•°")
        top_p: Optional[float] = Field(0.9, description="Top-pé‡‡æ ·")

    class ChatResponse(BaseModel):
        content: str
        model: str
        usage: Dict[str, int]
        finish_reason: Optional[str] = None

    class HealthResponse(BaseModel):
        status: str
        wrapper_status: str
        vllm_status: str
        model: str
        architecture: str

    # æ ¹è·¯å¾„
    @fastapi_app.get("/")
    async def root():
        return {
            "service": "VLLM Auto-Scale Service",
            "version": "2.0.0",
            "architecture": {
                "wrapper": "Always-on (no GPU)",
                "vllm": "Auto-scale to zero (with GPU)",
                "scaledown_window": "2 minutes"
            },
            "model": VLLM_MODEL,
            "endpoints": {
                "POST /chat": "å¯¹è¯æ¥å£",
                "POST /v1/chat/completions": "OpenAIå…¼å®¹æ¥å£",
                "GET /health": "å¥åº·æ£€æŸ¥",
            }
        }

    # å¥åº·æ£€æŸ¥
    @fastapi_app.get("/health", response_model=HealthResponse)
    async def health():
        return HealthResponse(
            status="healthy",
            wrapper_status="running",
            vllm_status="auto-scaling (idle or active)",
            model=VLLM_MODEL,
            architecture="separated"
        )

    # å¯¹è¯æ¥å£
    @fastapi_app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """
        å¯¹è¯æ¥å£ - è°ƒç”¨vLLMæ¨ç†å‡½æ•°

        æ³¨æ„ï¼šé¦–æ¬¡è°ƒç”¨å¯èƒ½éœ€è¦ç­‰å¾…vLLMå¯åŠ¨ï¼ˆå¦‚æœGPUå·²é‡Šæ”¾ï¼‰
        """
        try:
            logger.info(f"æ”¶åˆ°å¯¹è¯è¯·æ±‚ï¼Œ{len(request.messages)}æ¡æ¶ˆæ¯")

            # è°ƒç”¨vLLMæ¨ç†å‡½æ•°
            messages_dict = [{"role": m.role, "content": m.content} for m in request.messages]

            logger.info("æ­£åœ¨è°ƒç”¨vLLMæ¨ç†å‡½æ•°...")
            result = inference_cls.chat.remote(
                messages=messages_dict,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
            )

            logger.info("æ¨ç†å®Œæˆ")

            return ChatResponse(
                content=result["text"],
                model=VLLM_MODEL,
                usage={
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                },
                finish_reason=result["finish_reason"],
            )

        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    # OpenAIå…¼å®¹æ¥å£
    @fastapi_app.post("/v1/chat/completions")
    async def openai_chat(request: Dict):
        """OpenAIå…¼å®¹çš„å¯¹è¯æ¥å£"""
        try:
            messages = request.get("messages", [])
            max_tokens = request.get("max_tokens", 2048)
            temperature = request.get("temperature", 0.7)
            top_p = request.get("top_p", 0.9)

            # è°ƒç”¨vLLMæ¨ç†å‡½æ•°
            result = inference_cls.chat.remote(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )

            # OpenAIæ ¼å¼å“åº”
            return {
                "id": "chatcmpl-vllm",
                "object": "chat.completion",
                "created": int(__import__("time").time()),
                "model": VLLM_MODEL,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["text"],
                    },
                    "finish_reason": result["finish_reason"],
                }],
                "usage": {
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                }
            }

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    return fastapi_app


@app.local_entrypoint()
def main():
    """æœ¬åœ°å…¥å£"""
    print("=" * 70)
    print("ğŸš€ VLLM Auto-Scale æ¶æ„")
    print("=" * 70)
    print()
    print("ğŸ“¦ æ¶æ„è¯´æ˜:")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚  FastAPI Wrapper (æ°¸è¿œåœ¨çº¿)         â”‚")
    print("   â”‚  - æ— GPUï¼Œæˆæœ¬æä½                  â”‚")
    print("   â”‚  - å¤„ç†HTTPè¯·æ±‚                     â”‚")
    print("   â”‚  - å¯æ·»åŠ ç¼“å­˜/é™æµç­‰åŠŸèƒ½            â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("                  â”‚ Modal Function Call")
    print("                  â†“")
    print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("   â”‚  vLLMæ¨ç†å‡½æ•° (è‡ªåŠ¨ç¼©æ”¾)            â”‚")
    print("   â”‚  - æœ‰GPUï¼ŒæŒ‰éœ€ä½¿ç”¨                  â”‚")
    print("   â”‚  - 2åˆ†é’Ÿæ— è¯·æ±‚åé‡Šæ”¾GPU             â”‚")
    print("   â”‚  - æœ‰è¯·æ±‚æ—¶è‡ªåŠ¨å¯åŠ¨                 â”‚")
    print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("âš™ï¸  é…ç½®:")
    print(f"   - æ¨¡å‹: {VLLM_MODEL}")
    print(f"   - GPU: A100-80GB")
    print(f"   - Wrapperç¼©æ”¾: æ°¸ä¸é‡Šæ”¾")
    print(f"   - vLLMç¼©æ”¾: 2åˆ†é’Ÿåé‡Šæ”¾GPU")
    print()
    print("ğŸ’° æˆæœ¬ä¼˜åŠ¿:")
    print("   - Wrapper: ~$0.0001/å°æ—¶ (æ— GPU)")
    print("   - vLLM: ~$1.10/å°æ—¶ (ä»…åœ¨æ¨ç†æ—¶)")
    print("   - æ€»æˆæœ¬: å–å†³äºå®é™…ä½¿ç”¨é‡")
    print()
    print("ğŸ”§ éƒ¨ç½²å‘½ä»¤:")
    print("   modal deploy modal_vllm_autoscale.py")
    print()
    print("ğŸ“ éƒ¨ç½²å:")
    print("   - Wrapperç«‹å³å¯ç”¨ï¼Œæ— å†·å¯åŠ¨")
    print("   - é¦–æ¬¡æ¨ç†è¯·æ±‚ä¼šè§¦å‘vLLMå¯åŠ¨ï¼ˆçº¦1-2åˆ†é’Ÿï¼‰")
    print("   - åç»­è¯·æ±‚å¦‚æœåœ¨2åˆ†é’Ÿå†…ï¼Œç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹")
    print("   - 2åˆ†é’Ÿæ— è¯·æ±‚åï¼ŒGPUè‡ªåŠ¨é‡Šæ”¾")
    print("=" * 70)
