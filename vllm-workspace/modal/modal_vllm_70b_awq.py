"""
Modal éƒ¨ç½²ï¼šLlama-3.1-70B AWQé‡åŒ–ç‰ˆæœ¬

ä½¿ç”¨INT4 AWQé‡åŒ–ï¼Œå•ä¸ªA100-80GBå³å¯è¿è¡Œ
æ˜¾å­˜éœ€æ±‚ï¼š~55GBï¼ˆvs åŸå§‹ç‰ˆæœ¬çš„140GBï¼‰
æ€§èƒ½æŸå¤±ï¼š<5%
"""
import os
import modal
from typing import List, Dict, Any, Optional

# ===== é…ç½® =====
# ä½¿ç”¨é¢„é‡åŒ–çš„AWQæ¨¡å‹ï¼ˆæ¨èï¼‰
VLLM_MODEL = os.environ.get("VLLM_MODEL", "casperhansen/llama-3.1-70b-instruct-awq")
VLLM_MAX_MODEL_LEN = 4096  # AWQé‡åŒ–åå¯æ”¯æŒ4K context
VLLM_GPU_MEMORY_UTILIZATION = 0.90

# ===== Modal é•œåƒ =====
vllm_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git")
    .pip_install(
        "vllm==0.6.6.post1",
        "torch==2.5.1",
        "transformers==4.46.0",
        "hf-transfer",
        "autoawq",  # AWQé‡åŒ–æ”¯æŒ
    )
)

wrapper_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "fastapi>=0.109.0",
        "uvicorn[standard]>=0.27.0",
        "pydantic>=2.9",
    )
)

weights_volume = modal.Volume.from_name("vllm-70b-awq-cache", create_if_missing=True)

app = modal.App("vllm-70b-awq")

# ===== vLLM æ¨ç†å‡½æ•° =====
vllm_llm = None


@app.function(
    image=vllm_image,
    gpu="A100-80GB",  # å•å¡å³å¯ï¼
    volumes={"/weights": weights_volume},
    secrets=[modal.Secret.from_name("vllm-secrets")],
    scaledown_window=180,  # 3åˆ†é’Ÿåé‡Šæ”¾GPUï¼ˆ70BåŠ è½½æ…¢ï¼Œç»™å¤šç‚¹æ—¶é—´ï¼‰
    timeout=600,  # é¦–æ¬¡ä¸‹è½½æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
)
def generate_text_70b(
    messages: List[Dict[str, str]],
    max_tokens: int = 2048,
    temperature: float = 0.7,
    top_p: float = 0.9,
) -> Dict[str, Any]:
    """
    70B AWQé‡åŒ–æ¨¡å‹æ¨ç†

    æ˜¾å­˜ä¼˜åŒ–ï¼š
    - INT4é‡åŒ–ï¼šæ¨¡å‹æƒé‡ä»…35GBï¼ˆvs FP16çš„140GBï¼‰
    - å•A100-80GBå³å¯è¿è¡Œ
    - KVç¼“å­˜ï¼š~20GBï¼ˆ4K contextï¼‰
    - æ€»éœ€æ±‚ï¼š~55GB
    """
    global vllm_llm

    # é¦–æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–æ¨¡å‹
    if vllm_llm is None:
        from vllm import LLM, SamplingParams

        print(f"ğŸš€ Loading 70B AWQ model: {VLLM_MODEL}")
        print(f"ğŸ“Š Expected memory usage: ~55GB / 80GB")

        vllm_llm = LLM(
            model=VLLM_MODEL,
            download_dir="/weights",
            quantization="awq",  # å¯ç”¨AWQ INT4é‡åŒ–
            gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,
            max_model_len=VLLM_MAX_MODEL_LEN,
            tensor_parallel_size=1,  # å•GPU
            dtype="auto",  # è‡ªåŠ¨é€‰æ‹©æ•°æ®ç±»å‹
        )

        print(f"âœ… Model loaded successfully!")

    # æ„å»ºprompt
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]

        if role == "system":
            prompt += f"<|system|>\n{content}\n"
        elif role == "user":
            prompt += f"<|user|>\n{content}\n"
        elif role == "assistant":
            prompt += f"<|assistant|>\n{content}\n"

    prompt += "<|assistant|>\n"

    # æ¨ç†
    from vllm import SamplingParams

    sampling_params = SamplingParams(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )

    outputs = vllm_llm.generate([prompt], sampling_params)
    output = outputs[0]

    return {
        "text": output.outputs[0].text,
        "prompt_tokens": len(output.prompt_token_ids),
        "completion_tokens": len(output.outputs[0].token_ids),
        "finish_reason": output.outputs[0].finish_reason,
    }


# ===== FastAPI Wrapperï¼ˆæ°¸è¿œåœ¨çº¿ï¼‰=====
@app.function(image=wrapper_image)
@modal.asgi_app()
def wrapper():
    """FastAPI Wrapper - æ°¸è¿œåœ¨çº¿"""
    from fastapi import FastAPI, HTTPException, status
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, Field
    from typing import List, Dict, Optional
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    fastapi_app = FastAPI(
        title="VLLM 70B AWQ Service",
        description="Llama-3.1-70B-Instruct AWQé‡åŒ–ç‰ˆæœ¬ï¼ˆå•A100-80GBï¼‰",
        version="1.0.0",
    )

    fastapi_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # è¯·æ±‚/å“åº”æ¨¡å‹
    class Message(BaseModel):
        role: str
        content: str

    class ChatRequest(BaseModel):
        messages: List[Message]
        max_tokens: Optional[int] = 2048
        temperature: Optional[float] = 0.7
        top_p: Optional[float] = 0.9

    class ChatResponse(BaseModel):
        content: str
        model: str
        usage: Dict[str, int]
        finish_reason: Optional[str] = None

    class HealthResponse(BaseModel):
        status: str
        model: str
        quantization: str
        gpu_requirement: str
        memory_usage: str

    # APIç«¯ç‚¹
    @fastapi_app.get("/")
    async def root():
        return {
            "service": "VLLM 70B AWQ Service",
            "version": "1.0.0",
            "model": VLLM_MODEL,
            "quantization": "AWQ INT4",
            "gpu": "Single A100-80GB",
            "memory": "~55GB / 80GB",
            "quality_loss": "<5%",
            "endpoints": {
                "POST /chat": "å¯¹è¯æ¥å£",
                "POST /v1/chat/completions": "OpenAIå…¼å®¹æ¥å£",
                "GET /health": "å¥åº·æ£€æŸ¥",
            }
        }

    @fastapi_app.get("/health", response_model=HealthResponse)
    async def health():
        return HealthResponse(
            status="healthy",
            model=VLLM_MODEL,
            quantization="AWQ INT4",
            gpu_requirement="Single A100-80GB",
            memory_usage="~55GB / 80GB"
        )

    @fastapi_app.post("/chat", response_model=ChatResponse)
    async def chat(request: ChatRequest):
        """å¯¹è¯æ¥å£ - è°ƒç”¨70B AWQæ¨¡å‹"""
        try:
            logger.info(f"æ”¶åˆ°70Bæ¨¡å‹è¯·æ±‚ï¼Œ{len(request.messages)}æ¡æ¶ˆæ¯")

            messages_dict = [{"role": m.role, "content": m.content} for m in request.messages]

            logger.info("è°ƒç”¨70B AWQæ¨ç†å‡½æ•°...")

            # è°ƒç”¨Modalå‡½æ•°
            result = generate_text_70b.remote(
                messages=messages_dict,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
            )

            logger.info("æ¨ç†å®Œæˆ")

            return ChatResponse(
                content=result["text"],
                model=VLLM_MODEL,
                usage={
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                },
                finish_reason=result["finish_reason"],
            )

        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    @fastapi_app.post("/v1/chat/completions")
    async def openai_chat(request: Dict):
        """OpenAIå…¼å®¹æ¥å£"""
        try:
            messages = request.get("messages", [])
            max_tokens = request.get("max_tokens", 2048)
            temperature = request.get("temperature", 0.7)
            top_p = request.get("top_p", 0.9)

            result = generate_text_70b.remote(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
            )

            return {
                "id": "chatcmpl-vllm-70b",
                "object": "chat.completion",
                "created": int(__import__("time").time()),
                "model": VLLM_MODEL,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["text"],
                    },
                    "finish_reason": result["finish_reason"],
                }],
                "usage": {
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["prompt_tokens"] + result["completion_tokens"],
                }
            }

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ¨ç†å¤±è´¥: {str(e)}"
            )

    return fastapi_app


@app.local_entrypoint()
def main():
    print("=" * 70)
    print("ğŸš€ VLLM 70B AWQ é‡åŒ–éƒ¨ç½²")
    print("=" * 70)
    print()
    print("ğŸ“¦ æ¨¡å‹é…ç½®:")
    print(f"   - æ¨¡å‹: {VLLM_MODEL}")
    print(f"   - é‡åŒ–: AWQ INT4")
    print(f"   - GPU: å•ä¸ªA100-80GB")
    print(f"   - æ˜¾å­˜: ~55GB / 80GBï¼ˆèŠ‚çœ62%ï¼‰")
    print(f"   - ä¸Šä¸‹æ–‡: {VLLM_MAX_MODEL_LEN} tokens")
    print()
    print("âš¡ æ€§èƒ½ç‰¹ç‚¹:")
    print("   - è´¨é‡æŸå¤±: <5%")
    print("   - æ¨ç†é€Ÿåº¦: ~60 tokens/ç§’")
    print("   - æˆæœ¬: $1.10/å°æ—¶ï¼ˆvs 2Ã—GPU $2.20/å°æ—¶ï¼‰")
    print()
    print("ğŸ”§ éƒ¨ç½²å‘½ä»¤:")
    print("   modal deploy modal_vllm_70b_awq.py")
    print()
    print("ğŸ“ æ³¨æ„äº‹é¡¹:")
    print("   - é¦–æ¬¡ä¸‹è½½æ¨¡å‹éœ€è¦10-15åˆ†é’Ÿ")
    print("   - ç¡®ä¿HuggingFace tokenæœ‰æƒé™è®¿é—®æ¨¡å‹")
    print("   - 3åˆ†é’Ÿæ— è¯·æ±‚åGPUè‡ªåŠ¨é‡Šæ”¾")
    print()
    print("ğŸ’° æˆæœ¬å¯¹æ¯”:")
    print("   - åŸå§‹70Bï¼ˆ2Ã—A100ï¼‰: $2.20/å°æ—¶")
    print("   - AWQ 70Bï¼ˆ1Ã—A100ï¼‰: $1.10/å°æ—¶ â­")
    print("   - èŠ‚çœ: 50%")
    print("=" * 70)
