"""
Modal é›†æˆéƒ¨ç½²ï¼švLLM + FastAPI Wrapper

å°†vLLMæœåŠ¡å™¨å’ŒFastAPIåŒ…è£…å±‚éƒ¨ç½²åœ¨åŒä¸€ä¸ªå®¹å™¨ä¸­ï¼Œ
é¿å…å†·å¯åŠ¨é—®é¢˜ï¼Œä¿æŒæœåŠ¡æŒç»­è¿è¡Œã€‚
"""
import os
import subprocess
import time
import signal
from pathlib import Path
import modal

# ===== é…ç½® =====
# vLLM é…ç½®
VLLM_MODEL = os.environ.get("VLLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
VLLM_PORT = 8000
VLLM_MAX_MODEL_LEN = os.environ.get("VLLM_MAX_MODEL_LEN", "8192")
VLLM_GPU_MEMORY_UTILIZATION = os.environ.get("VLLM_GPU_MEMORY_UTILIZATION", "0.90")
VLLM_TENSOR_PARALLEL = int(os.environ.get("VLLM_TENSOR_PARALLEL", "1"))  # 8Bæ¨¡å‹å•GPUå³å¯

# FastAPI Wrapper é…ç½®
WRAPPER_PORT = 8001

# GPU é…ç½®
GPU_TYPE = os.environ.get("VLLM_GPU_TYPE", "A100-80GB")
GPU_COUNT = int(os.environ.get("VLLM_GPU_COUNT", "1"))  # 8Bæ¨¡å‹å•GPUè¶³å¤Ÿ

# è¶…æ—¶é…ç½®
CONTAINER_IDLE_TIMEOUT = 30 * 60  # 30åˆ†é’Ÿæ— è¯·æ±‚åä¼‘çœ 

# ===== Modal é•œåƒ =====
# é›†æˆé•œåƒï¼šåŒ…å«vLLMå’ŒFastAPIä¾èµ–
integrated_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git", "curl")
    .pip_install(
        # vLLM ä¾èµ– (vllm requires pydantic>=2.9)
        "vllm==0.6.6.post1",
        "torch==2.5.1",
        "transformers==4.46.0",
        "hf-transfer",
        # FastAPI ä¾èµ– (compatible versions)
        "fastapi>=0.109.0",
        "uvicorn[standard]>=0.27.0",
        "pydantic>=2.9",
        "openai>=1.54.0",
        "httpx>=0.27.0",
    )
)

# æ¨¡å‹ç¼“å­˜å·
weights_volume = modal.Volume.from_name("vllm-llama70b-cache", create_if_missing=True)

# Modal App
app = modal.App("vllm-integrated")


# ===== FastAPI Wrapper ä»£ç ï¼ˆå†…è”ç‰ˆæœ¬ï¼‰=====
WRAPPER_CODE = """
import os
import logging
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Header, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from openai import AsyncOpenAI, OpenAIError
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== é…ç½® =====
VLLM_BASE_URL = "http://localhost:8000/v1"
VLLM_MODEL = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-70B-Instruct")
VLLM_API_KEY = os.getenv("VLLM_SERVER_API_KEY", "")
SERVICE_API_KEY = os.getenv("VLLM_WRAPPER_API_KEY", "")

DEFAULT_MAX_TOKENS = 2048
DEFAULT_TEMPERATURE = 0.7

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ =====
class Message(BaseModel):
    role: str = Field(..., description="è§’è‰²: user, assistant, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")

class ChatRequest(BaseModel):
    messages: List[Message] = Field(..., description="å¯¹è¯å†å²")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•° (0-2)")
    top_p: Optional[float] = Field(None, description="Top-pé‡‡æ ·å‚æ•°")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼è¿”å›")
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰")

class ChatResponse(BaseModel):
    content: str = Field(..., description="å›å¤å†…å®¹")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨ç»Ÿè®¡")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")

class HealthResponse(BaseModel):
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    vllm_available: bool = Field(..., description="VLLMæ˜¯å¦å¯ç”¨")
    model: str = Field(..., description="é…ç½®çš„æ¨¡å‹")

# ===== VLLM å®¢æˆ·ç«¯ =====
class VLLMClient:
    def __init__(self):
        self.client = AsyncOpenAI(
            base_url=VLLM_BASE_URL,
            api_key=VLLM_API_KEY or "EMPTY",
            timeout=120.0
        )
        self.model = VLLM_MODEL
        logger.info(f"åˆå§‹åŒ–VLLMå®¢æˆ·ç«¯: {VLLM_BASE_URL}, æ¨¡å‹: {self.model}")

    async def health_check(self) -> bool:
        try:
            models = await self.client.models.list()
            logger.info(f"VLLMå¥åº·æ£€æŸ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"VLLMå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        stream: bool = False,
        model: Optional[str] = None,
    ):
        params = {
            "model": model or self.model,
            "messages": messages,
            "max_tokens": max_tokens or DEFAULT_MAX_TOKENS,
            "temperature": temperature if temperature is not None else DEFAULT_TEMPERATURE,
            "stream": stream,
        }
        if top_p is not None:
            params["top_p"] = top_p

        try:
            response = await self.client.chat.completions.create(**params)
            return response
        except OpenAIError as e:
            logger.error(f"VLLMè¯·æ±‚å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"VLLMæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"
            )

# ===== FastAPI åº”ç”¨ =====
vllm_client: Optional[VLLMClient] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global vllm_client
    logger.info("ğŸš€ VLLM Wrapper æœåŠ¡å¯åŠ¨")

    # ç­‰å¾…vLLMæœåŠ¡å°±ç»ª
    import asyncio
    max_retries = 30
    for i in range(max_retries):
        try:
            vllm_client = VLLMClient()
            if await vllm_client.health_check():
                logger.info("âœ… VLLM è¿æ¥æˆåŠŸ")
                break
        except Exception as e:
            if i < max_retries - 1:
                logger.info(f"ç­‰å¾…vLLMå¯åŠ¨... ({i+1}/{max_retries})")
                await asyncio.sleep(2)
            else:
                logger.warning("âš ï¸ VLLM è¿æ¥å¤±è´¥")

    yield
    logger.info("ğŸ‘‹ VLLM Wrapper æœåŠ¡å…³é—­")

app = FastAPI(
    title="VLLM Integrated Service",
    description="é›†æˆvLLMå’ŒFastAPIçš„ä¸€ä½“åŒ–æœåŠ¡",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== è®¤è¯ =====
def verify_api_key(authorization: Optional[str] = Header(None)):
    if not SERVICE_API_KEY:
        return True

    if not authorization:
        raise HTTPException(status_code=401, detail="Missing authorization header")

    token = authorization
    if authorization.startswith("Bearer "):
        token = authorization[7:]

    if token != SERVICE_API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API key")

    return True

# ===== API ç«¯ç‚¹ =====
@app.get("/")
async def root():
    return {
        "service": "VLLM Integrated Service",
        "version": "2.0.0",
        "status": "running",
        "model": VLLM_MODEL,
        "endpoints": {
            "POST /v1/chat/completions": "OpenAIå…¼å®¹å¯¹è¯æ¥å£",
            "POST /chat": "ç®€åŒ–å¯¹è¯æ¥å£",
            "GET /health": "å¥åº·æ£€æŸ¥",
            "GET /models": "åˆ—å‡ºæ¨¡å‹"
        }
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    if not vllm_client:
        return HealthResponse(
            status="starting",
            vllm_available=False,
            model=VLLM_MODEL
        )

    is_healthy = await vllm_client.health_check()
    return HealthResponse(
        status="healthy" if is_healthy else "degraded",
        vllm_available=is_healthy,
        model=VLLM_MODEL
    )

@app.get("/models")
async def list_models():
    if not vllm_client:
        raise HTTPException(status_code=503, detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

    try:
        models = await vllm_client.client.models.list()
        return {"models": [{"id": m.id, "object": m.object} for m in models.data]}
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {str(e)}")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    if not vllm_client:
        raise HTTPException(status_code=503, detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    if request.stream:
        async def generate():
            try:
                stream = await vllm_client.chat(
                    messages=messages,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                    top_p=request.top_p,
                    stream=True,
                    model=request.model
                )
                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield f"data: {chunk.choices[0].delta.content}\\n\\n"
                yield "data: [DONE]\\n\\n"
            except Exception as e:
                logger.error(f"æµå¼å“åº”é”™è¯¯: {e}")
                yield f"data: [ERROR] {str(e)}\\n\\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    response = await vllm_client.chat(
        messages=messages,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p,
        stream=False,
        model=request.model
    )

    choice = response.choices[0]
    return ChatResponse(
        content=choice.message.content,
        model=response.model,
        usage=response.usage.model_dump() if response.usage else None,
        finish_reason=choice.finish_reason
    )

@app.post("/v1/chat/completions")
async def openai_compatible_chat(request: Dict[str, Any]):
    if not vllm_client:
        raise HTTPException(status_code=503, detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

    try:
        stream = request.get("stream", False)
        response = await vllm_client.chat(
            messages=request.get("messages", []),
            max_tokens=request.get("max_tokens"),
            temperature=request.get("temperature"),
            top_p=request.get("top_p"),
            stream=stream,
            model=request.get("model")
        )

        if stream:
            async def generate():
                try:
                    async for chunk in response:
                        import json
                        yield f"data: {json.dumps(chunk.model_dump())}\\n\\n"
                    yield "data: [DONE]\\n\\n"
                except Exception as e:
                    logger.error(f"æµå¼å“åº”é”™è¯¯: {e}")

            return StreamingResponse(generate(), media_type="text/event-stream")

        return response.model_dump()

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¹è¯è¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
"""


@app.function(
    image=integrated_image,
    gpu="A100-80GB",  # å•ä¸ªA100 GPU for 8B model
    timeout=24 * 3600,  # 24å°æ—¶è¶…æ—¶
    scaledown_window=CONTAINER_IDLE_TIMEOUT,  # 30åˆ†é’Ÿæ— è¯·æ±‚åä¼‘çœ 
    volumes={"/weights": weights_volume},
    secrets=[modal.Secret.from_name("vllm-secrets")],
)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def serve():
    """
    å¯åŠ¨é›†æˆæœåŠ¡ï¼šåœ¨åŒä¸€ä¸ªå®¹å™¨ä¸­è¿è¡ŒvLLMå’ŒFastAPI Wrapper

    æ¶æ„ï¼š
    1. åå°å¯åŠ¨vLLMæœåŠ¡å™¨ (localhost:8000)
    2. å¯åŠ¨FastAPI Wrapper (ç›‘å¬å¤–éƒ¨è¯·æ±‚)
    3. FastAPIé€šè¿‡localhostè°ƒç”¨vLLMï¼Œæ— ç½‘ç»œå»¶è¿Ÿ
    """
    import sys
    import logging

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # å‡†å¤‡ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    hf_token = env.get("HUGGING_FACE_HUB_TOKEN") or env.get("HF_TOKEN")
    if hf_token:
        env["HUGGING_FACE_HUB_TOKEN"] = hf_token

    # è®¾ç½®vLLMæ¨¡å‹ç¯å¢ƒå˜é‡ä¾›wrapperä½¿ç”¨
    env["VLLM_MODEL"] = VLLM_MODEL

    # 1. åœ¨åå°å¯åŠ¨vLLMæœåŠ¡å™¨
    logger.info("=" * 60)
    logger.info("ğŸš€ å¯åŠ¨é›†æˆæœåŠ¡")
    logger.info("=" * 60)
    logger.info(f"ğŸ“¦ æ¨¡å‹: {VLLM_MODEL}")
    logger.info(f"ğŸ® GPU: {GPU_TYPE} x {GPU_COUNT}")
    logger.info(f"ğŸ’¾ æœ€å¤§é•¿åº¦: {VLLM_MAX_MODEL_LEN} tokens")
    logger.info(f"ğŸ”§ æ˜¾å­˜åˆ©ç”¨ç‡: {VLLM_GPU_MEMORY_UTILIZATION}")
    logger.info("=" * 60)

    vllm_command = [
        "python3", "-m", "vllm.entrypoints.openai.api_server",
        "--model", VLLM_MODEL,
        "--port", str(VLLM_PORT),
        "--host", "0.0.0.0",
        "--download-dir", "/weights",
        "--tensor-parallel-size", str(VLLM_TENSOR_PARALLEL),
        "--gpu-memory-utilization", str(VLLM_GPU_MEMORY_UTILIZATION),
        "--max-model-len", str(VLLM_MAX_MODEL_LEN),
    ]

    # æ·»åŠ API Keyï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    server_api_key = env.get("VLLM_SERVER_API_KEY")
    if server_api_key:
        vllm_command.extend(["--api-key", server_api_key])

    logger.info(f"ğŸ”¨ å¯åŠ¨vLLMæœåŠ¡å™¨: {' '.join(vllm_command)}")

    # åå°å¯åŠ¨vLLM
    vllm_process = subprocess.Popen(
        vllm_command,
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1
    )

    # 2. ç­‰å¾…vLLMæœåŠ¡å™¨å°±ç»ª
    logger.info("â³ ç­‰å¾…vLLMæœåŠ¡å™¨å¯åŠ¨...")
    max_wait = 300  # 5åˆ†é’Ÿ
    waited = 0
    vllm_ready = False

    # åœ¨åå°çº¿ç¨‹ä¸­è¯»å–vLLMè¾“å‡º
    import threading
    def log_vllm_output():
        for line in vllm_process.stdout:
            print(f"[vLLM] {line.rstrip()}", flush=True)

    log_thread = threading.Thread(target=log_vllm_output, daemon=True)
    log_thread.start()

    # æ£€æŸ¥vLLMæ˜¯å¦å°±ç»ª
    while waited < max_wait:
        try:
            import httpx
            response = httpx.get(f"http://localhost:{VLLM_PORT}/health", timeout=5.0)
            if response.status_code == 200:
                vllm_ready = True
                logger.info("âœ… vLLMæœåŠ¡å™¨å·²å°±ç»ª")
                break
        except Exception:
            pass

        time.sleep(5)
        waited += 5
        if waited % 30 == 0:
            logger.info(f"   ä»åœ¨ç­‰å¾…... ({waited}s / {max_wait}s)")

    if not vllm_ready:
        logger.error("âŒ vLLMæœåŠ¡å™¨å¯åŠ¨è¶…æ—¶")
        vllm_process.kill()
        raise RuntimeError("vLLMæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")

    # 3. åˆ›å»ºå¹¶è¿”å›FastAPIåº”ç”¨
    logger.info("ğŸŒ å¯åŠ¨FastAPI Wrapper...")

    # å°†wrapperä»£ç å†™å…¥ä¸´æ—¶æ–‡ä»¶
    wrapper_file = Path("/tmp/wrapper_app.py")
    wrapper_file.write_text(WRAPPER_CODE)

    # å¯¼å…¥FastAPIåº”ç”¨
    sys.path.insert(0, "/tmp")
    from wrapper_app import app as fastapi_app

    logger.info("=" * 60)
    logger.info("âœ… é›†æˆæœåŠ¡å¯åŠ¨å®Œæˆï¼")
    logger.info("=" * 60)
    logger.info(f"ğŸ”— vLLMæœåŠ¡: http://localhost:{VLLM_PORT}")
    logger.info(f"ğŸ”— FastAPIæœåŠ¡: ç›‘å¬Modalå¤–éƒ¨ç«¯å£")
    logger.info(f"â±ï¸  é—²ç½®è¶…æ—¶: {CONTAINER_IDLE_TIMEOUT // 60} åˆ†é’Ÿ")
    logger.info("=" * 60)

    return fastapi_app


@app.local_entrypoint()
def main():
    """æœ¬åœ°å…¥å£"""
    print("=" * 70)
    print("ğŸš€ VLLM é›†æˆéƒ¨ç½² - å°†vLLMå’ŒFastAPIéƒ¨ç½²åœ¨åŒä¸€å®¹å™¨ä¸­")
    print("=" * 70)
    print()
    print("ğŸ“¦ é…ç½®:")
    print(f"   - æ¨¡å‹: {VLLM_MODEL}")
    print(f"   - GPU: {GPU_TYPE} x {GPU_COUNT}")
    print(f"   - Tensor Parallel: {VLLM_TENSOR_PARALLEL}")
    print(f"   - æœ€å¤§ä¸Šä¸‹æ–‡: {VLLM_MAX_MODEL_LEN} tokens")
    print(f"   - æ˜¾å­˜åˆ©ç”¨ç‡: {VLLM_GPU_MEMORY_UTILIZATION}")
    print(f"   - é—²ç½®è¶…æ—¶: {CONTAINER_IDLE_TIMEOUT // 60} åˆ†é’Ÿ")
    print()
    print("ğŸ”§ éƒ¨ç½²å‘½ä»¤:")
    print("   modal deploy modal_vllm_integrated.py")
    print()
    print("ğŸ“ éœ€è¦é…ç½® Modal Secret 'vllm-secrets':")
    print("   - HUGGING_FACE_HUB_TOKEN: Hugging Faceè®¿é—®ä»¤ç‰Œï¼ˆå¿…éœ€ï¼‰")
    print("   - VLLM_SERVER_API_KEY: vLLMæœåŠ¡APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰")
    print("   - VLLM_WRAPPER_API_KEY: WrapperæœåŠ¡APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰")
    print()
    print("âœ¨ ä¼˜åŠ¿:")
    print("   âœ“ æ— å†·å¯åŠ¨é—®é¢˜ï¼ˆvLLMåœ¨å®¹å™¨å†…å¸¸é©»ï¼‰")
    print("   âœ“ æ— ç½‘ç»œå»¶è¿Ÿï¼ˆlocalhosté€šä¿¡ï¼‰")
    print("   âœ“ 30åˆ†é’Ÿæ— è¯·æ±‚åè‡ªåŠ¨ä¼‘çœ èŠ‚çœæˆæœ¬")
    print("   âœ“ ç»Ÿä¸€ç®¡ç†å’Œéƒ¨ç½²")
    print()
    print("ğŸ“¡ éƒ¨ç½²åçš„ç«¯ç‚¹:")
    print("   - POST /v1/chat/completions - OpenAIå…¼å®¹æ¥å£")
    print("   - POST /chat - ç®€åŒ–å¯¹è¯æ¥å£")
    print("   - GET /health - å¥åº·æ£€æŸ¥")
    print("   - GET /models - åˆ—å‡ºæ¨¡å‹")
    print("=" * 70)
