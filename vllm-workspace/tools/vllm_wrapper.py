"""
VLLM FastAPI åŒ…è£…æœåŠ¡
æä¾›ç®€åŒ–çš„å¯¹è¯æ¥å£ï¼Œæ”¯æŒæœ¬åœ°å’ŒModaléƒ¨ç½²
"""
import os
import logging
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Header, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from openai import AsyncOpenAI, OpenAIError
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===== é…ç½® =====
class VLLMConfig:
    """VLLM é…ç½®"""

    def __init__(self):
        # VLLM åç«¯é…ç½®
        self.VLLM_BASE_URL = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
        self.VLLM_MODEL = os.getenv("VLLM_MODEL", "meta-llama/Llama-3.1-70B-Instruct")
        self.VLLM_API_KEY = os.getenv("VLLM_API_KEY", "")

        # æœåŠ¡é…ç½®
        self.HOST = os.getenv("VLLM_WRAPPER_HOST", "0.0.0.0")
        self.PORT = int(os.getenv("VLLM_WRAPPER_PORT", "8001"))
        self.SERVICE_API_KEY = os.getenv("VLLM_WRAPPER_API_KEY", "")  # å¯é€‰çš„æœåŠ¡å±‚API Key

        # é»˜è®¤å‚æ•°
        self.DEFAULT_MAX_TOKENS = int(os.getenv("VLLM_DEFAULT_MAX_TOKENS", "2048"))
        self.DEFAULT_TEMPERATURE = float(os.getenv("VLLM_DEFAULT_TEMPERATURE", "0.7"))

        # é‡è¯•é…ç½®
        self.MAX_RETRIES = int(os.getenv("VLLM_MAX_RETRIES", "3"))
        self.TIMEOUT = int(os.getenv("VLLM_TIMEOUT", "60"))


config = VLLMConfig()


# ===== è¯·æ±‚/å“åº”æ¨¡å‹ =====
class Message(BaseModel):
    """å¯¹è¯æ¶ˆæ¯"""
    role: str = Field(..., description="è§’è‰²: user, assistant, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚"""
    messages: List[Message] = Field(..., description="å¯¹è¯å†å²")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    temperature: Optional[float] = Field(None, description="æ¸©åº¦å‚æ•° (0-2)")
    top_p: Optional[float] = Field(None, description="Top-pé‡‡æ ·å‚æ•°")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼è¿”å›")
    model: Optional[str] = Field(None, description="æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰")


class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”"""
    content: str = Field(..., description="å›å¤å†…å®¹")
    model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹")
    usage: Optional[Dict[str, int]] = Field(None, description="Tokenä½¿ç”¨ç»Ÿè®¡")
    finish_reason: Optional[str] = Field(None, description="ç»“æŸåŸå› ")


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    vllm_available: bool = Field(..., description="VLLMæ˜¯å¦å¯ç”¨")
    model: str = Field(..., description="é…ç½®çš„æ¨¡å‹")
    base_url: str = Field(..., description="VLLMæœåŠ¡åœ°å€")


# ===== VLLM å®¢æˆ·ç«¯ =====
class VLLMClient:
    """VLLM å®¢æˆ·ç«¯åŒ…è£…å™¨"""

    def __init__(self):
        self.client = AsyncOpenAI(
            base_url=config.VLLM_BASE_URL,
            api_key=config.VLLM_API_KEY or "EMPTY"
        )
        self.model = config.VLLM_MODEL
        logger.info(f"åˆå§‹åŒ–VLLMå®¢æˆ·ç«¯: {config.VLLM_BASE_URL}, æ¨¡å‹: {self.model}")

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            # å°è¯•åˆ—å‡ºæ¨¡å‹
            models = await self.client.models.list()
            logger.info(f"VLLMå¥åº·æ£€æŸ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {[m.id for m in models.data]}")
            return True
        except Exception as e:
            logger.error(f"VLLMå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        stream: bool = False,
        model: Optional[str] = None,
    ):
        """
        å‘é€å¯¹è¯è¯·æ±‚

        Args:
            messages: å¯¹è¯å†å²
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-på‚æ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            model: æ¨¡å‹åç§°

        Returns:
            OpenAI ChatCompletionå¯¹è±¡æˆ–æµå¼è¿­ä»£å™¨
        """
        params = {
            "model": model or self.model,
            "messages": messages,
            "max_tokens": max_tokens or config.DEFAULT_MAX_TOKENS,
            "temperature": temperature if temperature is not None else config.DEFAULT_TEMPERATURE,
            "stream": stream,
        }

        if top_p is not None:
            params["top_p"] = top_p

        logger.info(f"å‘é€VLLMè¯·æ±‚: {len(messages)}æ¡æ¶ˆæ¯, stream={stream}")

        try:
            response = await self.client.chat.completions.create(**params)
            return response
        except OpenAIError as e:
            logger.error(f"VLLMè¯·æ±‚å¤±è´¥: {e}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"VLLMæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"
            )
        except Exception as e:
            logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æœåŠ¡å†…éƒ¨é”™è¯¯: {str(e)}"
            )


# ===== FastAPI åº”ç”¨ =====
vllm_client: Optional[VLLMClient] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global vllm_client
    logger.info("ğŸš€ VLLM Wrapper æœåŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“ VLLM Base URL: {config.VLLM_BASE_URL}")
    logger.info(f"ğŸ¤– Model: {config.VLLM_MODEL}")

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    vllm_client = VLLMClient()

    # å¥åº·æ£€æŸ¥
    if await vllm_client.health_check():
        logger.info("âœ… VLLM è¿æ¥æˆåŠŸ")
    else:
        logger.warning("âš ï¸ VLLM è¿æ¥å¤±è´¥ï¼ŒæœåŠ¡å°†ç»§ç»­è¿è¡Œä½†å¯èƒ½æ— æ³•æ­£å¸¸å“åº”")

    yield

    logger.info("ğŸ‘‹ VLLM Wrapper æœåŠ¡å…³é—­")


app = FastAPI(
    title="VLLM Wrapper Service",
    description="VLLM å¯¹è¯æ¥å£åŒ…è£…æœåŠ¡ï¼Œæ”¯æŒæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®é…ç½®å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ===== è®¤è¯ä¸­é—´ä»¶ =====
def verify_api_key(authorization: Optional[str] = Header(None)):
    """éªŒè¯ API Keyï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰"""
    if not config.SERVICE_API_KEY:
        # æœªé…ç½®API Keyï¼Œè·³è¿‡éªŒè¯
        return True

    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Missing authorization header"
        )

    # æ”¯æŒ "Bearer <token>" æ ¼å¼
    token = authorization
    if authorization.startswith("Bearer "):
        token = authorization[7:]

    if token != config.SERVICE_API_KEY:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API key"
        )

    return True


# ===== API ç«¯ç‚¹ =====
@app.get("/")
async def root():
    """æœåŠ¡æ ¹è·¯å¾„"""
    return {
        "service": "VLLM Wrapper Service",
        "version": "1.0.0",
        "endpoints": {
            "POST /v1/chat/completions": "å¯¹è¯æ¥å£ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰",
            "POST /chat": "ç®€åŒ–å¯¹è¯æ¥å£",
            "GET /health": "å¥åº·æ£€æŸ¥",
            "GET /models": "åˆ—å‡ºå¯ç”¨æ¨¡å‹"
        }
    }


@app.get("/health", response_model=HealthResponse)
async def health():
    """å¥åº·æ£€æŸ¥"""
    if not vllm_client:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        )

    is_healthy = await vllm_client.health_check()

    return HealthResponse(
        status="healthy" if is_healthy else "degraded",
        vllm_available=is_healthy,
        model=config.VLLM_MODEL,
        base_url=config.VLLM_BASE_URL
    )


@app.get("/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    if not vllm_client:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        )

    try:
        models = await vllm_client.client.models.list()
        return {
            "models": [{"id": m.id, "object": m.object} for m in models.data]
        }
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {str(e)}"
        )


@app.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    _: bool = Header(None, alias="Authorization", include_in_schema=False, dependency=verify_api_key)
):
    """
    ç®€åŒ–çš„å¯¹è¯æ¥å£

    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    """
    if not vllm_client:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        )

    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    messages = [{"role": m.role, "content": m.content} for m in request.messages]

    # æµå¼å“åº”
    if request.stream:
        async def generate():
            try:
                stream = await vllm_client.chat(
                    messages=messages,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature,
                    top_p=request.top_p,
                    stream=True,
                    model=request.model
                )

                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        # SSE æ ¼å¼
                        yield f"data: {content}\n\n"

                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"æµå¼å“åº”é”™è¯¯: {e}")
                yield f"data: [ERROR] {str(e)}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream"
        )

    # éæµå¼å“åº”
    response = await vllm_client.chat(
        messages=messages,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p,
        stream=False,
        model=request.model
    )

    choice = response.choices[0]

    return ChatResponse(
        content=choice.message.content,
        model=response.model,
        usage=response.usage.model_dump() if response.usage else None,
        finish_reason=choice.finish_reason
    )


@app.post("/v1/chat/completions")
async def openai_compatible_chat(
    request: Dict[str, Any],
    _: bool = Header(None, alias="Authorization", include_in_schema=False, dependency=verify_api_key)
):
    """
    OpenAI å…¼å®¹çš„å¯¹è¯æ¥å£

    ç›´æ¥é€ä¼ åˆ°VLLMï¼Œä¿æŒå®Œå…¨å…¼å®¹æ€§
    """
    if not vllm_client:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="VLLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        )

    try:
        # ç›´æ¥è½¬å‘è¯·æ±‚åˆ°VLLM
        stream = request.get("stream", False)

        response = await vllm_client.chat(
            messages=request.get("messages", []),
            max_tokens=request.get("max_tokens"),
            temperature=request.get("temperature"),
            top_p=request.get("top_p"),
            stream=stream,
            model=request.get("model")
        )

        # æµå¼å“åº”
        if stream:
            async def generate():
                try:
                    async for chunk in response:
                        import json
                        yield f"data: {json.dumps(chunk.model_dump())}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logger.error(f"æµå¼å“åº”é”™è¯¯: {e}")

            return StreamingResponse(
                generate(),
                media_type="text/event-stream"
            )

        # éæµå¼å“åº”
        return response.model_dump()

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¹è¯è¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
        )


# ===== æœ¬åœ°è¿è¡Œå…¥å£ =====
if __name__ == "__main__":
    uvicorn.run(
        "vllm_wrapper:app",
        host=config.HOST,
        port=config.PORT,
        reload=True,
        log_level="info"
    )
