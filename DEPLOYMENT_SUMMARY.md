# VLLM éƒ¨ç½²å®Œæˆæ€»ç»“

## âœ… å·²å®Œæˆçš„ä»»åŠ¡

### 1. VLLM æœåŠ¡éƒ¨ç½²åˆ° Modal

**æœåŠ¡ä¿¡æ¯ï¼š**
- æœåŠ¡åœ°å€ï¼š`https://ybpang-1--vllm-llama70b-serve-vllm.modal.run`
- APIç«¯ç‚¹ï¼š`https://ybpang-1--vllm-llama70b-serve-vllm.modal.run/v1`
- æ¨¡å‹ï¼š`meta-llama/Llama-3.1-70B-Instruct`
- GPUï¼šA100-80GB x 1
- çŠ¶æ€ï¼šâœ… å·²éƒ¨ç½²

**Modal Secreté…ç½®ï¼š**
```bash
vllm-secrets:
  - HUGGING_FACE_HUB_TOKEN: <your-hf-token>
  - VLLM_SERVER_API_KEY: <your-vllm-api-key>
```

### 2. FastAPI VLLM åŒ…è£…æœåŠ¡

**åˆ›å»ºçš„æ–‡ä»¶ï¼ˆå·²ç§»åŠ¨åˆ° `vllm-workspace/`ï¼‰ï¼š**
- `vllm-workspace/tools/vllm_wrapper.py` - ä¸»æœåŠ¡ä»£ç 
- `vllm-workspace/modal/modal_vllm_wrapper.py` - Modaléƒ¨ç½²é…ç½®
- `vllm-workspace/tests/test_vllm_wrapper.py` - æµ‹è¯•è„šæœ¬
- `vllm-workspace/scripts/start_vllm_wrapper.sh` - æœ¬åœ°å¯åŠ¨è„šæœ¬
- `vllm-workspace/scripts/update_vllm_secret.sh` - Secretæ›´æ–°è„šæœ¬

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- âœ… ç®€åŒ–çš„å¯¹è¯æ¥å£ (`/chat`)
- âœ… OpenAI å…¼å®¹æ¥å£ (`/v1/chat/completions`)
- âœ… æµå¼å’Œéæµå¼å“åº”
- âœ… å¥åº·æ£€æŸ¥ (`/health`)
- âœ… æ¨¡å‹åˆ—è¡¨ (`/models`)
- âœ… API Key è®¤è¯ï¼ˆå¯é€‰ï¼‰
- âœ… è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- âœ… æ”¯æŒæœ¬åœ°å’Œäº‘ç«¯éƒ¨ç½²

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šæœ¬åœ°è¿è¡Œ VLLM Wrapper

```bash
# å¯åŠ¨æœåŠ¡
./vllm-workspace/scripts/start_vllm_wrapper.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼ˆä»ä»“åº“æ ¹ç›®å½•æ‰§è¡Œï¼‰
export VLLM_BASE_URL=https://ybpang-1--vllm-llama70b-serve-vllm.modal.run/v1
export VLLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
export VLLM_API_KEY=<your-vllm-api-key>
PYTHONPATH=vllm-workspace/tools python -m uvicorn vllm_wrapper:app --host 0.0.0.0 --port 8001
```

æœåŠ¡è®¿é—®ï¼š
- APIæ–‡æ¡£ï¼šhttp://localhost:8001/docs
- å¥åº·æ£€æŸ¥ï¼šhttp://localhost:8001/health

### æ–¹å¼äºŒï¼šéƒ¨ç½² VLLM Wrapper åˆ° Modal

```bash
# ç¡®ä¿ vllm-secrets åŒ…å«ä»¥ä¸‹é…ç½®ï¼š
# - VLLM_BASE_URL
# - VLLM_MODEL
# - VLLM_API_KEY
# - VLLM_WRAPPER_API_KEY (å¯é€‰)

modal deploy vllm-workspace/modal/modal_vllm_wrapper.py
```

---

## ğŸ“ API ä½¿ç”¨ç¤ºä¾‹

### 1. å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8001/health
```

**å“åº”ï¼š**
```json
{
  "status": "healthy",
  "vllm_available": true,
  "model": "meta-llama/Llama-3.1-70B-Instruct",
  "base_url": "https://ybpang-1--vllm-llama70b-serve-vllm.modal.run/v1"
}
```

### 2. å¯¹è¯è¯·æ±‚ï¼ˆéæµå¼ï¼‰

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

### 3. å¯¹è¯è¯·æ±‚ï¼ˆæµå¼ï¼‰

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "å†™ä¸€é¦–çŸ­è¯—"}
    ],
    "stream": true,
    "max_tokens": 200
  }'
```

### 4. Python å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import httpx
import asyncio

async def chat():
    url = "http://localhost:8001/chat"
    payload = {
        "messages": [
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
        ],
        "max_tokens": 500,
        "temperature": 0.7
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(url, json=payload)
        result = response.json()
        print(f"å›å¤: {result['content']}")

asyncio.run(chat())
```

### 5. ä½¿ç”¨ OpenAI SDK

```python
from openai import AsyncOpenAI

async def chat_with_openai_sdk():
    client = AsyncOpenAI(
        base_url="http://localhost:8001/v1",
        api_key="dummy"  # å¦‚æœæ²¡é…ç½®è®¤è¯å¯ä»¥éšä¾¿å¡«
    )

    response = await client.chat.completions.create(
        model="meta-llama/Llama-3.1-70B-Instruct",
        messages=[
            {"role": "user", "content": "Hello!"}
        ]
    )

    print(response.choices[0].message.content)

import asyncio
asyncio.run(chat_with_openai_sdk())
```

---

## ğŸ”§ æµ‹è¯•

### è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶

```bash
python test_vllm_wrapper.py
```

æµ‹è¯•å°†éªŒè¯ï¼š
- âœ… æœåŠ¡å¥åº·çŠ¶æ€
- âœ… æ¨¡å‹åˆ—è¡¨è·å–
- âœ… éæµå¼å¯¹è¯
- âœ… æµå¼å¯¹è¯

---

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### Modal ç›¸å…³å‘½ä»¤

```bash
# æŸ¥çœ‹åº”ç”¨çŠ¶æ€
modal app list

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
modal app logs vllm-llama70b

# åœæ­¢æœåŠ¡
modal app stop vllm-llama70b

# é‡æ–°éƒ¨ç½²
modal deploy vllm-workspace/modal/modal_vllm.py

# æŸ¥çœ‹ Secret
modal secret list
```

### æœ¬åœ°æœåŠ¡ç®¡ç†

```bash
# æŸ¥çœ‹8001ç«¯å£å ç”¨
lsof -i:8001

# åœæ­¢æœåŠ¡
pkill -f "uvicorn vllm_wrapper"

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼ˆå¦‚æœåå°è¿è¡Œï¼‰
tail -f vllm_wrapper.log
```

---

## âš ï¸ é‡è¦æç¤º

### 1. VLLM å†·å¯åŠ¨

é¦–æ¬¡è¯·æ±‚VLLMæœåŠ¡æ—¶éœ€è¦2-5åˆ†é’ŸåŠ è½½70Bæ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚æœåŠ¡ä¼šæ˜¾ç¤ºï¼š
```
âš ï¸ VLLM è¿æ¥å¤±è´¥ï¼ŒæœåŠ¡å°†ç»§ç»­è¿è¡Œä½†å¯èƒ½æ— æ³•æ­£å¸¸å“åº”
```

è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆåå³å¯æ­£å¸¸ä½¿ç”¨ã€‚

### 2. æˆæœ¬æ§åˆ¶

- A100-80GB GPUï¼šçº¦ $3.12/å°æ—¶
- 15åˆ†é’Ÿæ— è¯·æ±‚è‡ªåŠ¨ä¼‘çœ 
- ä»…åœ¨è¿è¡Œæ—¶è®¡è´¹

### 3. è®¿é—®æƒé™

ç¡®ä¿å·²æ¥å— Llama 3.1 æ¨¡å‹çš„ä½¿ç”¨åè®®ï¼š
https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct

### 4. API Key å®‰å…¨

- `VLLM_API_KEY`: ç”¨äºè®¿é—®Modalä¸Šçš„VLLMæœåŠ¡
- `VLLM_WRAPPER_API_KEY`: ç”¨äºä¿æŠ¤ä½ çš„åŒ…è£…æœåŠ¡ï¼ˆå¯é€‰ï¼‰

å¦‚éœ€åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œè¯·è®¾ç½®`VLLM_WRAPPER_API_KEY`å¹¶åœ¨æ‰€æœ‰è¯·æ±‚ä¸­æ·»åŠ ï¼š
```bash
Authorization: Bearer your-wrapper-api-key
```

---

## ğŸ“– æ–‡æ¡£

- **VLLMéƒ¨ç½²æŒ‡å—**: [VLLM_MODAL_DEPLOYMENT.md](./VLLM_MODAL_DEPLOYMENT.md)
- **åŒ…è£…æœåŠ¡æŒ‡å—**: [VLLM_WRAPPER_GUIDE.md](./VLLM_WRAPPER_GUIDE.md)
- **Modalæ–‡æ¡£**: https://modal.com/docs
- **OpenAI APIå‚è€ƒ**: https://platform.openai.com/docs/api-reference

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### é›†æˆåˆ°ç°æœ‰åç«¯

åœ¨ä½ çš„ `backend/main.py` ä¸­æ·»åŠ ï¼š

```python
import httpx

async def call_vllm(messages: list):
    """é€šè¿‡ VLLM Wrapper è°ƒç”¨ LLM"""
    url = "http://localhost:8001/chat"
    payload = {
        "messages": messages,
        "max_tokens": 1024,
        "temperature": 0.7
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(url, json=payload)
        return response.json()["content"]
```

### æ€§èƒ½ä¼˜åŒ–

1. **å¯ç”¨è¿æ¥æ± **: httpxä¼šè‡ªåŠ¨ç®¡ç†
2. **æ·»åŠ ç¼“å­˜**: å¸¸è§å¯¹è¯å¯ä»¥ç¼“å­˜åˆ°Redis
3. **è´Ÿè½½å‡è¡¡**: éƒ¨ç½²å¤šä¸ªVLLMå®ä¾‹
4. **ç›‘æ§**: æ·»åŠ PrometheusæŒ‡æ ‡

### æ‰©å±•åŠŸèƒ½

1. **å¯¹è¯å†å²ç®¡ç†**: å®ç°ä¼šè¯å­˜å‚¨
2. **å¤šæ¨¡å‹æ”¯æŒ**: æ·»åŠ å…¶ä»–Llamaæ¨¡å‹
3. **é€Ÿç‡é™åˆ¶**: ä½¿ç”¨fastapi-limiter
4. **WebSocketæ”¯æŒ**: å®ç°å®æ—¶æµå¼å¯¹è¯

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ï¼šVLLMè¿æ¥å¤±è´¥

**æ£€æŸ¥æ¸…å•ï¼š**
- [ ] ModalæœåŠ¡æ˜¯å¦åœ¨è¿è¡Œ
- [ ] VLLM_BASE_URLæ˜¯å¦æ­£ç¡®
- [ ] VLLM_API_KEYæ˜¯å¦åŒ¹é…
- [ ] ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- [ ] æ¨¡å‹æ˜¯å¦å®ŒæˆåŠ è½½ï¼ˆå†·å¯åŠ¨éœ€è¦å‡ åˆ†é’Ÿï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ModalæœåŠ¡çŠ¶æ€
modal app list | grep vllm

# æŸ¥çœ‹VLLMæœåŠ¡æ—¥å¿—
modal app logs vllm-llama70b

# é‡æ–°éƒ¨ç½²
modal deploy vllm-workspace/modal/modal_vllm.py
```

### é—®é¢˜ï¼š401 Unauthorized

**å¯èƒ½åŸå› ï¼š**
1. VLLM_API_KEY ä¸åŒ¹é…
2. Hugging Face Token æ— æ•ˆæˆ–è¿‡æœŸ
3. æœªæ¥å—æ¨¡å‹è®¸å¯åè®®

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ›´æ–°Modal Secret
./vllm-workspace/scripts/update_vllm_secret.sh

# é‡æ–°éƒ¨ç½²
modal deploy vllm-workspace/modal/modal_vllm.py
```

### é—®é¢˜ï¼šè¯·æ±‚è¶…æ—¶

**å¯èƒ½åŸå› ï¼š**
1. VLLMæ­£åœ¨å†·å¯åŠ¨
2. max_tokensè®¾ç½®è¿‡å¤§
3. æ¨¡å‹è´Ÿè½½è¿‡é«˜

**è§£å†³æ–¹æ¡ˆï¼š**
- ç­‰å¾…2-5åˆ†é’Ÿè®©æ¨¡å‹å®Œå…¨åŠ è½½
- å‡å°‘max_tokenså‚æ•°
- æ£€æŸ¥ModalæœåŠ¡è´Ÿè½½

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
1. æœåŠ¡æ—¥å¿—ï¼š`modal app logs vllm-llama70b`
2. å¥åº·æ£€æŸ¥ï¼š`curl http://localhost:8001/health`
3. APIæ–‡æ¡£ï¼šhttp://localhost:8001/docs

---

## âœ… éªŒè¯æ¸…å•

éƒ¨ç½²å®Œæˆåï¼Œç¡®è®¤ä»¥ä¸‹åŠŸèƒ½æ­£å¸¸ï¼š

- [ ] Modal VLLMæœåŠ¡è¿è¡Œæ­£å¸¸
- [ ] VLLM WrapperæœåŠ¡å¯åŠ¨æˆåŠŸ
- [ ] å¥åº·æ£€æŸ¥è¿”å›healthy
- [ ] å¯ä»¥åˆ—å‡ºæ¨¡å‹
- [ ] éæµå¼å¯¹è¯æ­£å¸¸
- [ ] æµå¼å¯¹è¯æ­£å¸¸
- [ ] APIè®¤è¯å·¥ä½œæ­£å¸¸ï¼ˆå¦‚é…ç½®ï¼‰
- [ ] ä¸ç°æœ‰åç«¯é›†æˆæˆåŠŸ

---

**éƒ¨ç½²å®Œæˆæ—¶é—´**: 2025-12-03
**éƒ¨ç½²çŠ¶æ€**: âœ… æˆåŠŸ
**æœåŠ¡ç‰ˆæœ¬**: 1.0.0
